#!/usr/bin/env python3
"""
VRçœ¼åŠ¨æ•°æ®è‡ªåŠ¨åˆ†æå·¥å…· / VR Gaze Data Auto Analysis Tool

åŠŸèƒ½è¯´æ˜ï¼š
- è‡ªåŠ¨æ£€æµ‹è§†é¢‘ä¸­çš„ç™½è‰²åœ†å½¢è§†çº¿ç‚¹
- åˆ†æè§†çº¿ç‚¹å‘¨å›´åŒºåŸŸåˆ¤æ–­ç°å®ä¸–ç•Œvsè™šæ‹Ÿä¸–ç•Œ
- ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Šå’Œæ—¶é—´æ®µç»Ÿè®¡
- æ”¯æŒå®æ—¶é¢„è§ˆå’Œæ‰¹é‡å¤„ç†

ä½œè€…ï¼šWeihao
ç‰ˆæœ¬ï¼š1.0
æ–‡ä»¶åï¼šgaze_analyzer.py
"""
import cv2
import numpy as np
import os
import pandas as pd
from collections import defaultdict, Counter, deque
import argparse
import json
import glob

class GazeAnalyzer:
    def __init__(self):
        # æ£€æµ‹å‚æ•°
        self.black_threshold = 30  # é»‘è‰²é˜ˆå€¼ï¼ˆ0-255ï¼‰
        self.pure_black_threshold = 15  # pure-black threshold for strict screen region detection
        self.detection_radius = 20  # è§†çº¿ç‚¹å‘¨å›´æ£€æµ‹åŠå¾„
        self.reality_mask_min_coverage = 0.4  # minimum mask coverage required to consider reality
        self.reality_black_ratio_threshold = 0.55  # minimum proportion of dark pixels in ROI
        self.reality_brightness_scale = 0.9  # scale factor for black-threshold brightness check
        self.manual_tolerance_px = 8  # pixel tolerance when reconciling manual adjustments
        self.side_ui_exclusion_ratio = 0.12  # portion of frame width to ignore for side UI panels
        self.min_duration = 5  # æœ€å°æŒç»­å¸§æ•°ï¼ˆé¿å…å™ªå£°ï¼‰
        
        # æ˜¾ç¤ºå‚æ•°
        self.indicator_size = (100, 80)  # æŒ‡ç¤ºå™¨å¤§å°
        self.indicator_pos = (20, 20)   # æŒ‡ç¤ºå™¨ä½ç½®
        
        # çŠ¶æ€è¿½è¸ª
        self.current_state = None  # 'reality' or 'virtual'
        self.state_start_frame = 0
        self.segments = []  # å­˜å‚¨æ‰€æœ‰ç‰‡æ®µ
        
        # è¿‘å¤„ä¼˜å…ˆæ£€æµ‹
        self.last_gaze_position = None  # ä¸Šä¸€å¸§çš„è§†çº¿ä½ç½®
        self.proximity_radius = 128     # è¿‘å¤„æœç´¢åŠå¾„
        
        # åœ†å½¢è´¨é‡æ§åˆ¶å‚æ•°
        self.min_circle_fill_ratio = 0.55
        self.max_circle_std_ratio = 0.6
        self.max_ring_intensity_gap = 25
        self.min_perimeter_brightness_ratio = 0.7
        self.max_color_std_for_circle = 35.0
        self.max_perimeter_radius_std = 0.3
        self.max_eccentricity_ratio = 1.8
        # åœºæ™¯åˆ¤æ–­å‚æ•°
        self.scene_dark_ratio_real_min = 0.72
        self.scene_edge_real_max = 0.04
        self.scene_color_std_real_max = 12.0
        self.scene_largest_real_min = 0.55
        self.scene_edge_virtual_min = 0.08
        self.scene_sat_virtual_min = 28.0
        self.scene_color_std_virtual_min = 20.0
        self.scene_dark_virtual_max = 0.5


        # çŠ¶æ€ç¨³å®šæ§åˆ¶
        self.transition_hold_frames = 3
        self.pending_state = None
        self.pending_start_frame = 0

        # åœºæ™¯å¹³æ»‘å†å²
        self.scene_vote_history = deque(maxlen=15)

    def load_model(self, model_path):
        """Load threshold overrides from a JSON profile"""
        if not model_path:
            return
        if not os.path.exists(model_path):
            print(f"[WARN] Model file not found: {model_path}")
            return
        try:
            with open(model_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as exc:
            print(f"[WARN] Failed to load model {model_path}: {exc}")
            return

        mapping = {
            'min_circle_fill_ratio': 'min_circle_fill_ratio',
            'max_circle_std_ratio': 'max_circle_std_ratio',
            'max_ring_intensity_gap': 'max_ring_intensity_gap',
            'min_perimeter_brightness_ratio': 'min_perimeter_brightness_ratio',
            'max_color_std_for_circle': 'max_color_std_for_circle',
            'max_perimeter_radius_std': 'max_perimeter_radius_std',
            'max_eccentricity_ratio': 'max_eccentricity_ratio',
            'transition_hold_frames': 'transition_hold_frames',
<<<<<<< HEAD
            'scene_dark_ratio_real_min': 'scene_dark_ratio_real_min',
            'scene_edge_real_max': 'scene_edge_real_max',
            'scene_color_std_real_max': 'scene_color_std_real_max',
            'scene_largest_real_min': 'scene_largest_real_min',
            'scene_edge_virtual_min': 'scene_edge_virtual_min',
            'scene_sat_virtual_min': 'scene_sat_virtual_min',
            'scene_color_std_virtual_min': 'scene_color_std_virtual_min',
            'scene_dark_virtual_max': 'scene_dark_virtual_max',
=======
            'reality_mask_min_coverage': 'reality_mask_min_coverage',
            'reality_black_ratio_threshold': 'reality_black_ratio_threshold',
            'reality_brightness_scale': 'reality_brightness_scale'
>>>>>>> ML
        }

        for key, attr in mapping.items():
            if key in data:
                setattr(self, attr, data[key])
        print(f"[INFO] Loaded gaze model from {model_path}")


    def detect_gaze_circle(self, frame):
        """Detect bright circular gaze point"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        h, w = gray.shape
        top_exclude = int(h * 0.05)
        left_exclude = int(w * 0.23)
        right_exclude = w - int(w * 0.23)

        black_mask = self.create_black_region_mask(gray)

        if self.last_gaze_position is not None:
            proximity_circle = self.detect_with_proximity_priority(frame, gray, left_exclude, right_exclude, top_exclude, black_mask)
            if proximity_circle:
                self.last_gaze_position = (proximity_circle[0], proximity_circle[1])
                return proximity_circle, black_mask

        circles = cv2.HoughCircles(
            gray,
            cv2.HOUGH_GRADIENT,
            dp=1,
            minDist=30,
            param1=60,
            param2=35,
            minRadius=3,
            maxRadius=12
        )

        black_region_circle = self.detect_in_black_region(frame, gray, black_mask, left_exclude, right_exclude, top_exclude)
        if black_region_circle:
            self.last_gaze_position = (black_region_circle[0], black_region_circle[1])
            return black_region_circle, black_mask

        if circles is None:
            avg_brightness = np.mean(gray)
            if avg_brightness < 80:
                circles = cv2.HoughCircles(
                    gray,
                    cv2.HOUGH_GRADIENT,
                    dp=1,
                    minDist=25,
                    param1=40,
                    param2=20,
                    minRadius=3,
                    maxRadius=12
                )

        if circles is not None:
            circles = np.round(circles[0, :]).astype("int")
            frame_avg_brightness = float(np.mean(gray))

            best_circle = None
            max_score = -float('inf')

            for (x, y, r) in circles:
                if not (left_exclude <= x <= right_exclude and y >= top_exclude and 0 <= x < w and 0 <= y < h):
                    continue

                if self.is_steering_wheel_button(gray, x, y, r):
                    continue

                context = "black_region" if black_mask is not None and black_mask[min(max(y, 0), h - 1), min(max(x, 0), w - 1)] > 0 else "default"
                metrics = self.evaluate_circle_candidate(frame, gray, x, y, r, context=context)
                if metrics is None:
                    continue

                brightness = metrics["mean"]
                contrast = metrics["contrast"]

                if frame_avg_brightness < 80:
                    score = brightness * 0.4 + contrast * 0.6
                    if brightness > 150 and contrast > 50:
                        score += 50
                else:
                    score = brightness * 0.7 + contrast * 0.3

                if score > max_score:
                    max_score = score
                    best_circle = (x, y, r)

            if best_circle is not None:
                self.last_gaze_position = (best_circle[0], best_circle[1])
                return best_circle, black_mask

        return None, black_mask
    
    def is_steering_wheel_button(self, gray, x, y, r):
        """æ£€æµ‹æ˜¯å¦æ˜¯æ–¹å‘ç›˜æŒ‰é’®ï¼ˆç™½è‰²è¾¹ç•Œ+é»‘è‰²å†…éƒ¨ï¼‰"""
        # æ£€æŸ¥åœ†å¿ƒåŒºåŸŸ
        center_r = max(1, int(r * 0.6))  # å†…éƒ¨åŒºåŸŸåŠå¾„
        center_roi = gray[max(0, y-center_r):min(gray.shape[0], y+center_r),
                         max(0, x-center_r):min(gray.shape[1], x+center_r)]
        
        # æ£€æŸ¥è¾¹ç•ŒåŒºåŸŸ
        edge_r = r
        edge_roi = gray[max(0, y-edge_r):min(gray.shape[0], y+edge_r),
                       max(0, x-edge_r):min(gray.shape[1], x+edge_r)]
        
        if center_roi.size == 0 or edge_roi.size == 0:
            return False
        
        center_brightness = np.mean(center_roi)
        edge_brightness = np.mean(edge_roi)
        
        # æ–¹å‘ç›˜ç‰¹å¾ï¼šè¾¹ç•Œäº®ï¼ˆç™½è‰²ï¼‰ï¼Œä¸­å¿ƒæš—ï¼ˆé»‘è‰²æŒ‰é’®ï¼‰
        # çœŸå®è§†çº¿ç‚¹ç‰¹å¾ï¼šæ•´ä½“éƒ½æ˜¯ç™½è‰²ï¼Œä¸­å¿ƒä¹Ÿå¾ˆäº®
        is_steering_wheel = (
            edge_brightness > 120 and          # è¾¹ç•Œè¾ƒäº®ï¼ˆç™½è‰²è¾¹ç•Œï¼‰
            center_brightness < 80 and         # ä¸­å¿ƒè¾ƒæš—ï¼ˆé»‘è‰²æŒ‰é’®ï¼‰
            (edge_brightness - center_brightness) > 60  # è¾¹ç•Œä¸ä¸­å¿ƒå¯¹æ¯”åº¦é«˜
        )
        
        return is_steering_wheel
    def evaluate_circle_candidate(self, frame, gray, x, y, r, context="default"):
        """Compute metrics for a circle candidate"""
        h, w = gray.shape
        radius = int(max(2, round(r)))
        if radius <= 1:
            return None

        pad = max(radius + 2, int(radius * 1.5))
        x1 = max(0, x - pad)
        y1 = max(0, y - pad)
        x2 = min(w, x + pad + 1)
        y2 = min(h, y + pad + 1)
        roi_gray = gray[y1:y2, x1:x2]
        if roi_gray.size == 0:
            return None

        center = (x - x1, y - y1)
        context = context or "default"

        if context == "black_region":
            fill_ratio_factor = 0.35
            min_fill_ratio = max(0.4, self.min_circle_fill_ratio - 0.1)
            max_std_ratio = self.max_circle_std_ratio + 0.2
            ring_gap_limit = self.max_ring_intensity_gap + 8
            min_perimeter_ratio = max(0.55, self.min_perimeter_brightness_ratio - 0.15)
            color_std_limit = self.max_color_std_for_circle + 15
            min_mean_intensity = 85
            perimeter_std_limit = self.max_perimeter_radius_std * 1.35
            eccentricity_limit = self.max_eccentricity_ratio + 0.4
        else:
            fill_ratio_factor = 0.45
            min_fill_ratio = self.min_circle_fill_ratio
            max_std_ratio = self.max_circle_std_ratio
            ring_gap_limit = self.max_ring_intensity_gap
            min_perimeter_ratio = self.min_perimeter_brightness_ratio
            color_std_limit = self.max_color_std_for_circle
            min_mean_intensity = 90
            perimeter_std_limit = self.max_perimeter_radius_std
            eccentricity_limit = self.max_eccentricity_ratio

        mask = np.zeros_like(roi_gray, dtype=np.uint8)
        cv2.circle(mask, center, radius, 255, -1)
        circle_pixels = roi_gray[mask == 255]
        min_pixels = max(8, int(np.pi * radius * radius * fill_ratio_factor))
        if circle_pixels.size < min_pixels:
            return None

        circle_pixels = circle_pixels.astype(np.float32)
        mean_intensity = float(np.mean(circle_pixels))
        if mean_intensity < min_mean_intensity:
            return None

        std_intensity = float(np.std(circle_pixels))
        std_ratio = std_intensity / (mean_intensity + 1e-6)

        inner_r = max(1, int(radius * 0.55))
        inner_mask = np.zeros_like(mask, dtype=np.uint8)
        cv2.circle(inner_mask, center, inner_r, 255, -1)
        inner_pixels = roi_gray[inner_mask == 255]
        inner_mean = float(np.mean(inner_pixels)) if inner_pixels.size > 0 else mean_intensity

        ring_mask = np.zeros_like(mask, dtype=np.uint8)
        cv2.circle(ring_mask, center, radius, 255, -1)
        ring_inner_r = max(inner_r, int(radius * 0.75))
        cv2.circle(ring_mask, center, ring_inner_r, 0, -1)
        ring_pixels = roi_gray[ring_mask == 255]
        ring_mean = float(np.mean(ring_pixels)) if ring_pixels.size > 0 else mean_intensity

        _, binary = cv2.threshold(roi_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        fill_pixels = int(np.sum((binary == 255) & (mask == 255)))
        fill_ratio = fill_pixels / max(1, circle_pixels.size)

        perimeter_mask = np.zeros_like(mask, dtype=np.uint8)
        cv2.circle(perimeter_mask, center, radius, 255, 1)
        perimeter_pixels = roi_gray[perimeter_mask == 255].astype(np.float32)
        perimeter_threshold = mean_intensity - (10.0 if context == "black_region" else 12.0)
        perimeter_ratio = float(np.mean(perimeter_pixels > perimeter_threshold)) if perimeter_pixels.size > 0 else 1.0

        perimeter_coords = np.column_stack(np.where(perimeter_mask == 255))
        if perimeter_coords.size > 0:
            dy = perimeter_coords[:, 0].astype(np.float32) - center[1]
            dx = perimeter_coords[:, 1].astype(np.float32) - center[0]
            distances = np.sqrt(dx * dx + dy * dy)
            perimeter_std = float(np.std(distances)) / (radius + 1e-6)
        else:
            perimeter_std = 0.0

        surround_x1 = max(0, x - radius * 2)
        surround_y1 = max(0, y - radius * 2)
        surround_x2 = min(w, x + radius * 2 + 1)
        surround_y2 = min(h, y + radius * 2 + 1)
        surrounding_roi = gray[surround_y1:surround_y2, surround_x1:surround_x2]
        contrast = float(mean_intensity - np.mean(surrounding_roi)) if surrounding_roi.size > 0 else 0.0

        color_std = None
        if frame is not None:
            roi_color = frame[y1:y2, x1:x2]
            if roi_color.size > 0:
                circle_colors = roi_color[mask == 255].astype(np.float32)
                if circle_colors.size > 0:
                    color_std = float(np.mean(np.std(circle_colors, axis=0)))

        ring_gap = ring_mean - inner_mean

        if fill_ratio < min_fill_ratio:
            if not (context == "black_region" and fill_ratio > min_fill_ratio - 0.08 and mean_intensity > 150):
                return None

        if perimeter_ratio < min_perimeter_ratio:
            return None

        if perimeter_std > perimeter_std_limit:
            return None

        if ring_gap > ring_gap_limit and inner_mean < 180:
            return None

        if std_ratio > max_std_ratio and fill_ratio < 0.9:
            return None

        if color_std is not None and color_std > color_std_limit and fill_ratio < 0.85:
            return None

        coords = np.column_stack(np.where((binary == 255) & (mask == 255)))
        if coords.size > 1:
            rel_y = coords[:, 0].astype(np.float32) - center[1]
            rel_x = coords[:, 1].astype(np.float32) - center[0]
            cov = np.cov(np.stack([rel_x, rel_y]))
            eigen_vals = np.linalg.eigvalsh(cov + 1e-6 * np.eye(2))
            ecc_ratio = float(eigen_vals.max() / (eigen_vals.min() + 1e-6))
        else:
            ecc_ratio = 1.0

        if ecc_ratio > eccentricity_limit:
            return None

        return {
            "mean": mean_intensity,
            "contrast": contrast,
            "fill_ratio": fill_ratio,
            "std_ratio": std_ratio,
            "ring_diff": ring_gap,
            "perimeter_ratio": perimeter_ratio,
            "perimeter_std": perimeter_std,
            "color_std": color_std,
            "inner_mean": inner_mean,
            "ring_mean": ring_mean,
            "eccentricity": ecc_ratio,
        }
    def detect_with_proximity_priority(self, frame, gray, left_exclude, right_exclude, top_exclude, black_mask=None):
        """Prioritize search near the last gaze position"""
        if self.last_gaze_position is None:
            return None

        last_x, last_y = self.last_gaze_position
        h, w = gray.shape

        for search_radius in [128, 256, 384]:
            search_x1 = max(left_exclude, last_x - search_radius)
            search_y1 = max(top_exclude, last_y - search_radius)
            search_x2 = min(right_exclude, last_x + search_radius)
            search_y2 = min(h, last_y + search_radius)

            if search_x2 - search_x1 < 50 or search_y2 - search_y1 < 50:
                continue

            search_roi = gray[search_y1:search_y2, search_x1:search_x2]

            circles = cv2.HoughCircles(
                search_roi,
                cv2.HOUGH_GRADIENT,
                dp=1,
                minDist=20,
                param1=35,
                param2=25,
                minRadius=3,
                maxRadius=12
            )

            if circles is None:
                continue

            circles = np.round(circles[0, :]).astype("int")

            best_circle = None
            min_distance = float('inf')

            for (rel_x, rel_y, r) in circles:
                abs_x = rel_x + search_x1
                abs_y = rel_y + search_y1

                if not (left_exclude <= abs_x <= right_exclude and abs_y >= top_exclude):
                    continue

                if self.is_steering_wheel_button(gray, abs_x, abs_y, r):
                    continue

                if black_mask is not None:
                    mask_y = int(np.clip(abs_y, 0, h - 1))
                    mask_x = int(np.clip(abs_x, 0, w - 1))
                    context = "black_region" if black_mask[mask_y, mask_x] > 0 else "default"
                else:
                    context = "default"

                metrics = self.evaluate_circle_candidate(frame, gray, abs_x, abs_y, r, context=context)
                if metrics is None:
                    continue

                distance = ((abs_x - last_x) ** 2 + (abs_y - last_y) ** 2) ** 0.5

                if metrics["mean"] > 100 and distance < min_distance:
                    min_distance = distance
                    best_circle = (abs_x, abs_y, r)

            if best_circle is not None:
                return best_circle

        return None

    def _refine_mask(self, mask, kernel_size=5, close_iters=1, open_iters=1):
        """Apply simple morphology to clean binary masks."""
        if mask is None:
            return None
        kernel = np.ones((kernel_size, kernel_size), np.uint8)
        if close_iters > 0:
            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=close_iters)
        if open_iters > 0:
            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=open_iters)
        return mask

    def _apply_side_exclusion(self, mask):
        """Zero-out side UI panels based on configured ratio."""
        if mask is None:
            return None
        h, w = mask.shape[:2]
        margin = int(round(w * self.side_ui_exclusion_ratio))
        if margin <= 0:
            return mask
        mask = mask.copy()
        mask[:, :margin] = 0
        mask[:, w - margin:] = 0
        return mask

    def _bridge_linear_gaps(self, mask, gap_ratio=0.02, orientation='vertical'):
        """Fill narrow bright streaks that cut through dark regions."""
        if mask is None:
            return None
        h, w = mask.shape[:2]
        if orientation == 'vertical':
            kernel_width = max(3, int(w * gap_ratio))
            if kernel_width % 2 == 0:
                kernel_width += 1
            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_width, 1))
        else:
            kernel_height = max(3, int(h * gap_ratio))
            if kernel_height % 2 == 0:
                kernel_height += 1
            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, kernel_height))
        return cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=1)

    def create_pure_black_mask(self, gray):
        """Return a mask of near-zero-intensity pixels (pure black)."""
        if gray is None:
            return None
        if len(gray.shape) == 3:
            gray = cv2.cvtColor(gray, cv2.COLOR_BGR2GRAY)
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        threshold = max(0, min(self.pure_black_threshold, self.black_threshold))
        mask = cv2.inRange(blurred, 0, threshold)
        mask = self._refine_mask(mask, kernel_size=3, close_iters=2, open_iters=1)
        if mask is None:
            return None
        if cv2.countNonZero(mask) == 0:
            return mask
        mask = cv2.dilate(mask, np.ones((3, 3), np.uint8), iterations=1)
        mask = self._bridge_linear_gaps(mask, gap_ratio=0.015, orientation='vertical')
        mask = self._apply_side_exclusion(mask)
        return mask

    def create_black_region_mask(self, gray):
        """Build mask of dark regions, prioritising pure-black areas."""
        if gray is None:
            return None
        if len(gray.shape) == 3:
            gray = cv2.cvtColor(gray, cv2.COLOR_BGR2GRAY)
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        area_threshold = max(200, int(gray.shape[0] * gray.shape[1] * 0.001))
        pure_mask = self.create_pure_black_mask(gray)
        pure_area = cv2.countNonZero(pure_mask) if pure_mask is not None else 0

        adaptive = cv2.adaptiveThreshold(
            blurred,
            255,
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            cv2.THRESH_BINARY_INV,
            21,
            7
        )
        _, otsu = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        range_mask = cv2.inRange(blurred, 0, self.black_threshold)

        combined = cv2.bitwise_or(adaptive, otsu)
        combined = cv2.bitwise_and(combined, range_mask)
        combined = self._refine_mask(combined, kernel_size=5, close_iters=2, open_iters=1)
        combined = self._apply_side_exclusion(combined)

        mask = np.zeros_like(gray)
        contours, _ = cv2.findContours(combined, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        for contour in contours:
            if cv2.contourArea(contour) < area_threshold:
                continue
            component_mask = np.zeros_like(gray)
            cv2.drawContours(component_mask, [contour], -1, 255, -1)
            mean_intensity = cv2.mean(blurred, mask=component_mask)[0]
            if mean_intensity <= self.black_threshold:
                cv2.drawContours(mask, [contour], -1, 255, -1)

        if pure_area > 0 and pure_mask is not None:
            mask = cv2.bitwise_or(mask, pure_mask)
            mask = self._refine_mask(mask, kernel_size=5, close_iters=1, open_iters=1)

        final_area = cv2.countNonZero(mask)
        if final_area < area_threshold and pure_area >= area_threshold:
            mask = pure_mask
        elif final_area == 0:
            mask = combined

        mask = self._bridge_linear_gaps(mask, gap_ratio=0.015, orientation='vertical')
        mask = self._apply_side_exclusion(mask)
        return mask

    def detect_in_black_region(self, frame, gray, black_mask, left_exclude, right_exclude, top_exclude):
        """Detect bright dots inside dark regions"""
        if black_mask is None:
            return None

        circles = cv2.HoughCircles(
            gray,
            cv2.HOUGH_GRADIENT,
            dp=1,
            minDist=15,
            param1=25,
            param2=15,
            minRadius=3,
            maxRadius=12
        )

        if circles is None:
            return None

        circles = np.round(circles[0, :]).astype("int")

        best_circle = None
        max_score = 0.0

        for (x, y, r) in circles:
            if not (left_exclude <= x <= right_exclude and y >= top_exclude):
                continue

            mask_y = int(np.clip(y, 0, gray.shape[0] - 1))
            mask_x = int(np.clip(x, 0, gray.shape[1] - 1))
            if black_mask[mask_y, mask_x] == 0:
                continue

            if self.is_steering_wheel_button(gray, x, y, r):
                continue

            metrics = self.evaluate_circle_candidate(frame, gray, x, y, r, context="black_region")
            if metrics is None:
                continue

            brightness = metrics["mean"]
            contrast = metrics["contrast"]

            if brightness < 110:
                continue

            score = brightness * 0.6 + contrast * 2.4

            if score > max_score:
                max_score = score
                best_circle = (x, y, r)

        return best_circle

<<<<<<< HEAD
    def analyze_gaze_region(self, frame, gaze_x, gaze_y, black_mask=None, scene_features=None):
        """Classify whether the gaze region looks real-world or virtual"""
=======
    def analyze_gaze_region(self, frame, gaze_x, gaze_y, black_mask=None, pure_mask=None):
        """Classify gaze region with strict black-mask requirements"""
>>>>>>> ML
        h, w = frame.shape[:2]

        x1 = max(0, gaze_x - self.detection_radius)
        y1 = max(0, gaze_y - self.detection_radius)
        x2 = min(w, gaze_x + self.detection_radius)
        y2 = min(h, gaze_y + self.detection_radius)

        roi = frame[y1:y2, x1:x2]
        if roi.size == 0:
<<<<<<< HEAD
            return 'unknown'

        gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        avg_brightness = float(np.mean(gray_roi))
        edge_roi = cv2.Canny(gray_roi, 40, 120)
        edge_density = float(np.mean(edge_roi > 0))

        mask_ratio = 0.0
        if black_mask is not None:
            mask_roi = black_mask[y1:y2, x1:x2]
            if mask_roi.size > 0:
                mask_ratio = float(np.mean(mask_roi > 0))

        bottom_dark = scene_features.get('bottom_dark_ratio', 0.0) if scene_features else 0.0
        bottom_mask = scene_features.get('bottom_mask_ratio', 0.0) if scene_features else 0.0
        bottom_edge = scene_features.get('bottom_edge_density', 0.0) if scene_features else 0.0

        if bottom_dark > self.scene_dark_ratio_real_min and bottom_mask > 0.5 and bottom_edge < 0.04:
            if edge_density < 0.06 and avg_brightness < 130:
                return 'reality'

        if scene_features and scene_features.get('scene_guess') == 'reality' and edge_density < 0.06:
            return 'reality'

        if edge_density > 0.1 or avg_brightness > 150:
            return 'virtual'

        if scene_features:
            return scene_features.get('scene_guess', 'virtual')
        return 'virtual'

    def compute_scene_features(self, frame, black_mask=None):
        """Extract global features for scene classification"""
        h, w = frame.shape[:2]
        features = {}

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        edges_full = cv2.Canny(gray, 60, 160)
        features['edge_density_full'] = float(np.mean(edges_full > 0))

        dark_mask = gray < 45
        features['dark_ratio_full'] = float(np.mean(dark_mask))

        top_h = int(h * 0.55)
        top_roi = frame[:top_h, :]
        gray_top = gray[:top_h, :]
        edges_top = edges_full[:top_h, :]
        hsv_top = cv2.cvtColor(top_roi, cv2.COLOR_BGR2HSV)

        features['edge_density_top'] = float(np.mean(edges_top > 0))
        features['dark_ratio_top'] = float(np.mean(gray_top < 45))
        features['sat_mean_top'] = float(np.mean(hsv_top[:, :, 1]))
        features['color_std_top'] = float(np.mean(np.std(top_roi.reshape(-1, 3), axis=0)))

        bottom_roi = frame[top_h:, :]
        gray_bottom = gray[top_h:, :]
        edges_bottom = edges_full[top_h:, :]
        features['bottom_dark_ratio'] = float(np.mean(gray_bottom < 45))
        features['bottom_edge_density'] = float(np.mean(edges_bottom > 0))

        if black_mask is not None:
            mask_bool = black_mask > 0
            features['mask_ratio_full'] = float(np.mean(mask_bool))
            contours, _ = cv2.findContours(black_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if contours:
                areas = [cv2.contourArea(c) for c in contours]
                features['largest_region_ratio'] = float(max(areas)) / float(h * w)
            else:
                features['largest_region_ratio'] = 0.0
            bottom_mask = black_mask[top_h:, :]
            features['bottom_mask_ratio'] = float(np.mean(bottom_mask > 0))
=======
            return 'virtual'

        mask_y = int(np.clip(gaze_y, 0, h - 1))
        mask_x = int(np.clip(gaze_x, 0, w - 1))

        in_pure = False
        in_black = False
        if pure_mask is not None and pure_mask.shape[:2] == (h, w):
            in_pure = pure_mask[mask_y, mask_x] > 0
        if black_mask is not None and black_mask.shape[:2] == (h, w):
            in_black = black_mask[mask_y, mask_x] > 0

        if not (in_pure or in_black):
            return 'virtual'

        gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        avg_brightness = float(np.mean(gray_roi))
        black_pixels = np.sum(gray_roi < self.black_threshold)
        total_pixels = gray_roi.size
        black_ratio = (black_pixels / total_pixels) if total_pixels else 0.0

        coverage = 0.0
        if black_mask is not None and black_mask.shape[:2] == (h, w):
            mask_roi = black_mask[y1:y2, x1:x2]
            if mask_roi.size:
                coverage = max(coverage, float(np.mean(mask_roi > 0)))
        if pure_mask is not None and pure_mask.shape[:2] == (h, w):
            pure_roi = pure_mask[y1:y2, x1:x2]
            if pure_roi.size:
                coverage = max(coverage, float(np.mean(pure_roi > 0)))

        if coverage < self.reality_mask_min_coverage:
            return 'virtual'

        brightness_threshold = self.black_threshold * self.reality_brightness_scale
        if black_ratio >= self.reality_black_ratio_threshold or avg_brightness <= brightness_threshold:
            return 'reality'

        return 'virtual'

    def draw_indicator(self, frame, state):
        """åœ¨å·¦ä¸Šè§’ç»˜åˆ¶çŠ¶æ€æŒ‡ç¤ºå™¨"""
        x, y = self.indicator_pos
        w, h = self.indicator_size
        
        # é€‰æ‹©é¢œè‰²
        if state == 'reality':
            color = (0, 255, 0)  # ç»¿è‰²
            text = 'REALITY'
        elif state == 'virtual':
            color = (0, 0, 255)  # çº¢è‰²
            text = 'VIRTUAL'
>>>>>>> ML
        else:
            features['mask_ratio_full'] = 0.0
            features['largest_region_ratio'] = 0.0
            features['bottom_mask_ratio'] = 0.0

        features['scene_guess'] = self.estimate_scene_state(features)
        return features

    def estimate_scene_state(self, features, default='virtual'):
        """Guess overall scene using extracted features"""
        if not features:
            return default

        dark_top = features.get('dark_ratio_top', 0.0)
        edge_top = features.get('edge_density_top', 0.0)
        color_std = features.get('color_std_top', 0.0)
        sat_top = features.get('sat_mean_top', 0.0)
        largest = features.get('largest_region_ratio', 0.0)
        bottom_dark = features.get('bottom_dark_ratio', 0.0)
        bottom_edge = features.get('bottom_edge_density', 0.0)
        bottom_mask = features.get('bottom_mask_ratio', 0.0)

        if bottom_dark > self.scene_dark_ratio_real_min and bottom_mask > 0.55 and bottom_edge < 0.04:
            return 'reality'
        if dark_top > self.scene_dark_ratio_real_min and edge_top < self.scene_edge_real_max and color_std < self.scene_color_std_real_max and bottom_edge < 0.05:
            return 'reality'
        if largest > self.scene_largest_real_min and edge_top < self.scene_edge_real_max and color_std < self.scene_color_std_real_max:
            return 'reality'

        if edge_top > self.scene_edge_virtual_min or sat_top > self.scene_sat_virtual_min or color_std > self.scene_color_std_virtual_min:
            return 'virtual'
        if bottom_dark < self.scene_dark_virtual_max or bottom_edge > 0.08:
            return 'virtual'

        return default
    def draw_mask_indicator(self, frame, source_frame, black_mask, scene_features=None):
        """Render frame/mask comparison thumbnail with stats"""
        if black_mask is None:
            return

        h, w = frame.shape[:2]

        thumb_w = max(80, min(160, w // 8))
        thumb_h = max(60, min(120, int(thumb_w * 0.75)))

        mask_resized = cv2.resize(black_mask, (thumb_w, thumb_h))
        frame_resized = cv2.resize(source_frame, (thumb_w, thumb_h))

        mask_overlay = cv2.merge([np.zeros_like(mask_resized), mask_resized, np.zeros_like(mask_resized)])
        overlay = cv2.addWeighted(frame_resized, 0.55, mask_overlay, 0.45, 0)
        mask_colored = cv2.cvtColor(mask_resized, cv2.COLOR_GRAY2BGR)

        combined = np.hstack([frame_resized, overlay, mask_colored])
        total_w = combined.shape[1]

        thumb_x = 20
        if thumb_x + total_w > w - 10:
            thumb_x = max(10, w - total_w - 10)
        thumb_y = h - thumb_h - 20

        if thumb_y < 0 or thumb_x < 0:
            return

        frame[thumb_y:thumb_y + thumb_h, thumb_x:thumb_x + total_w] = combined

        cv2.rectangle(frame, (thumb_x, thumb_y), (thumb_x + total_w, thumb_y + thumb_h), (255, 255, 255), 1)
        font = cv2.FONT_HERSHEY_SIMPLEX
        cv2.putText(frame, 'Frame | Overlay | Mask', (thumb_x, thumb_y - 5), font, 0.4, (255, 255, 255), 1)

        if scene_features:
            dark_top = scene_features.get('dark_ratio_top', 0.0)
            edge_top = scene_features.get('edge_density_top', 0.0)
            bottom_dark = scene_features.get('bottom_dark_ratio', 0.0)
            bottom_mask = scene_features.get('bottom_mask_ratio', 0.0)
            sat_top = scene_features.get('sat_mean_top', 0.0)
            largest = scene_features.get('largest_region_ratio', 0.0)
            guess = scene_features.get('scene_guess', 'n/a').upper()
            label1 = f"DarkTop:{dark_top:.2f} EdgeTop:{edge_top:.2f}"
            label2 = f"BottomDark:{bottom_dark:.2f} BottomMask:{bottom_mask:.2f}"
            label3 = f"Largest:{largest:.2f} SatTop:{sat_top:.1f} Scene:{guess}"
            cv2.putText(frame, label1, (thumb_x + 5, thumb_y + thumb_h - 33), font, 0.42, (200, 255, 200), 1)
            cv2.putText(frame, label2, (thumb_x + 5, thumb_y + thumb_h - 17), font, 0.42, (200, 255, 200), 1)
            cv2.putText(frame, label3, (thumb_x + 5, thumb_y + thumb_h - 1), font, 0.42, (200, 255, 200), 1)
    def update_state(self, raw_state, frame_num, fps):
        """Update stable state with hysteresis"""
        if raw_state == 'unknown':
            raw_state = self.current_state if self.current_state is not None else 'virtual'

        if self.current_state is None:
            self.current_state = raw_state
            self.state_start_frame = frame_num
            self.pending_state = None
            self.pending_start_frame = frame_num
            return

        if raw_state == self.current_state:
            self.pending_state = None
            return

        if self.pending_state != raw_state:
            self.pending_state = raw_state
            self.pending_start_frame = frame_num
            return

        if frame_num - self.pending_start_frame + 1 < self.transition_hold_frames:
            return

        transition_frame = self.pending_start_frame
        if transition_frame > self.state_start_frame:
            duration_frames = transition_frame - self.state_start_frame
            if duration_frames >= self.min_duration:
                duration_seconds = duration_frames / fps
                self.segments.append({
                    'state': self.current_state,
                    'start_frame': self.state_start_frame,
                    'end_frame': transition_frame - 1,
                    'duration_frames': duration_frames,
                    'duration_seconds': duration_seconds,
                    'start_time': self.state_start_frame / fps,
                    'end_time': (transition_frame - 1) / fps
                })

        self.current_state = self.pending_state
        self.state_start_frame = transition_frame
        self.pending_state = None
    
    def finalize_segments(self, total_frames, fps):
        """å®Œæˆæœ€åä¸€ä¸ªç‰‡æ®µçš„è®°å½•"""
        if self.current_state is not None and total_frames - self.state_start_frame >= self.min_duration:
            duration_frames = total_frames - self.state_start_frame
            duration_seconds = duration_frames / fps
            
            self.segments.append({
                'state': self.current_state,
                'start_frame': self.state_start_frame,
                'end_frame': total_frames - 1,
                'duration_frames': duration_frames,
                'duration_seconds': duration_seconds,
                'start_time': self.state_start_frame / fps,
                'end_time': (total_frames - 1) / fps
            })

    def analyze_video(self, video_path, output_dir=None, show_preview=True):
        """Analyze a single video"""
        print(f"ğŸ¬ å¼€å§‹åˆ†æ: {os.path.basename(video_path)}")

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            return None

        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps:.2f}fps, {total_frames}å¸§")

        self.segments = []
        self.current_state = None
        self.pending_state = None
        self.pending_start_frame = 0

        # åœºæ™¯å¹³æ»‘å†å²
        self.scene_vote_history = deque(maxlen=15)
        self.last_gaze_position = None

        frame_num = 0

        output_video = None
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            output_video_path = os.path.join(output_dir, f"{os.path.splitext(os.path.basename(video_path))[0]}_analyzed.mp4")
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                raw_frame_for_mask = frame.copy()
                gray_frame = cv2.cvtColor(raw_frame_for_mask, cv2.COLOR_BGR2GRAY)
                pure_black_mask = self.create_pure_black_mask(gray_frame)

                detection_result, black_mask = self.detect_gaze_circle(frame)
                scene_features = self.compute_scene_features(frame, black_mask)
                scene_guess = scene_features.get('scene_guess', 'virtual')
                scene_guess = self.update_scene_history(scene_guess)
                scene_features['scene_guess'] = scene_guess

<<<<<<< HEAD
                if detection_result:
                    gaze_x, gaze_y, radius = detection_result
                    raw_state = self.analyze_gaze_region(frame, gaze_x, gaze_y, black_mask, scene_features)
                    cv2.circle(frame, (gaze_x, gaze_y), radius, (255, 255, 0), 2)
                    cv2.circle(frame, (gaze_x, gaze_y), self.detection_radius, (0, 255, 255), 1)
                else:
                    raw_state = scene_guess
=======
                raw_state = 'virtual'
                gaze_circle = None
                black_mask = None

                if detection_result and len(detection_result) == 2:
                    gaze_circle, black_mask = detection_result

                if gaze_circle:
                    gaze_x, gaze_y, radius = gaze_circle
                    raw_state = self.analyze_gaze_region(
                        frame,
                        gaze_x,
                        gaze_y,
                        black_mask=black_mask,
                        pure_mask=pure_black_mask
                    )
                    cv2.circle(frame, (gaze_x, gaze_y), radius, (255, 255, 0), 2)
                    cv2.circle(frame, (gaze_x, gaze_y), self.detection_radius, (0, 255, 255), 1)
>>>>>>> ML

                self.update_state(raw_state, frame_num, fps)
                stable_state = self.current_state if self.current_state is not None else raw_state

                self.apply_reality_overlay(frame, black_mask, stable_state)
                self.draw_indicator(frame, stable_state)

<<<<<<< HEAD
                if black_mask is not None:
                    self.draw_mask_indicator(frame, raw_frame_for_mask, black_mask, scene_features)
=======
                mask_for_overlay = black_mask if black_mask is not None else pure_black_mask
                if mask_for_overlay is not None:
                    self.draw_mask_indicator(frame, raw_frame_for_mask, mask_for_overlay)
>>>>>>> ML

                if frame_num % 100 == 0 and total_frames > 0:
                    progress = (frame_num / total_frames) * 100
                    print(f"â³ å¤„ç†è¿›åº¦: {progress:.1f}% ({frame_num}/{total_frames})")

                if output_video:
                    output_video.write(frame)

                if show_preview:
                    display_frame = frame
                    if width > 1280:
                        scale = 1280 / width
                        display_frame = cv2.resize(frame, (int(width * scale), int(height * scale)))

                    cv2.imshow('Gaze Analysis', display_frame)

                    if cv2.waitKey(1) & 0xFF == 27:
                        print("â¹ï¸  ç”¨æˆ·ä¸­æ–­é¢„è§ˆ")
                        break

                frame_num += 1

        finally:
            cap.release()
            if output_video:
                output_video.release()
            if show_preview:
                cv2.destroyAllWindows()

        self.finalize_segments(frame_num, fps)

        print(f"âœ… åˆ†æå®Œæˆ! å…±å¤„ç† {frame_num} å¸§")

        self.generate_report(video_path, output_dir)

        return self.segments
    def generate_report(self, video_path, output_dir):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not self.segments:
            print("âš ï¸  æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆç‰‡æ®µ")
            return
        
        # ç»Ÿè®¡æ•°æ®
        reality_segments = [s for s in self.segments if s['state'] == 'reality']
        virtual_segments = [s for s in self.segments if s['state'] == 'virtual']
        
        reality_duration = sum(s['duration_seconds'] for s in reality_segments)
        virtual_duration = sum(s['duration_seconds'] for s in virtual_segments)
        total_duration = reality_duration + virtual_duration
        
        print(f"\nğŸ“Š åˆ†ææŠ¥å‘Š:")
        print(f"=" * 50)
        print(f"ç°å®ä¸–ç•Œç‰‡æ®µ: {len(reality_segments)} ä¸ª, æ€»æ—¶é•¿: {reality_duration:.2f}ç§’")
        print(f"è™šæ‹Ÿä¸–ç•Œç‰‡æ®µ: {len(virtual_segments)} ä¸ª, æ€»æ—¶é•¿: {virtual_duration:.2f}ç§’")
        
        if total_duration > 0:
            print(f"ç°å®ä¸–ç•Œå æ¯”: {(reality_duration/total_duration*100):.1f}%")
            print(f"è™šæ‹Ÿä¸–ç•Œå æ¯”: {(virtual_duration/total_duration*100):.1f}%")
        
        # ä¿å­˜è¯¦ç»†æ•°æ®
        if output_dir:
            # åˆ›å»ºDataFrame
            df_data = []
            for i, segment in enumerate(self.segments, 1):
                df_data.append({
                    'åºå·': i,
                    'çŠ¶æ€': 'ç°å®ä¸–ç•Œ' if segment['state'] == 'reality' else 'è™šæ‹Ÿä¸–ç•Œ',
                    'å¼€å§‹å¸§': segment['start_frame'],
                    'ç»“æŸå¸§': segment['end_frame'],
                    'æŒç»­å¸§æ•°': segment['duration_frames'],
                    'å¼€å§‹æ—¶é—´(ç§’)': round(segment['start_time'], 2),
                    'ç»“æŸæ—¶é—´(ç§’)': round(segment['end_time'], 2),
                    'æŒç»­æ—¶é—´(ç§’)': round(segment['duration_seconds'], 2)
                })
            
            df = pd.DataFrame(df_data)
            
            # ä¿å­˜CSVæ–‡ä»¶
            base_name = os.path.splitext(os.path.basename(video_path))[0]
            csv_path = os.path.join(output_dir, f"{base_name}_analysis.csv")
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            
            print(f"ğŸ“„ è¯¦ç»†æ•°æ®å·²ä¿å­˜: {csv_path}")

def get_video_files(directory):
    """è·å–ç›®å½•ä¸‹çš„è§†é¢‘æ–‡ä»¶"""
    video_extensions = ['*.mp4', '*.avi', '*.mov', '*.mkv', '*.flv']
    video_files = []
    
    for ext in video_extensions:
        video_files.extend(glob.glob(os.path.join(directory, '**', ext), recursive=True))
    
    return sorted(video_files)

def main():
    parser = argparse.ArgumentParser(description="VR gaze analyzer")
    parser.add_argument("--input", "-i", default="çœ¼åŠ¨æ•°æ®", help="è¾“å…¥ç›®å½• (é»˜è®¤: çœ¼åŠ¨æ•°æ®)")
    parser.add_argument("--output", "-o", default="analysis_results", help="è¾“å‡ºç›®å½• (é»˜è®¤: analysis_results)")
    parser.add_argument("--no-preview", action="store_true", help="ä¸æ˜¾ç¤ºå®æ—¶é¢„è§ˆ")
    parser.add_argument("--black-threshold", type=int, default=30, help="é»‘è‰²æ£€æµ‹é˜ˆå€¼ (é»˜è®¤: 30)")
    parser.add_argument("--radius", type=int, default=20, help="æ£€æµ‹åŠå¾„ (é»˜è®¤: 20)")
    parser.add_argument("--model", type=str, help="åŠ è½½è®­ç»ƒé˜ˆå€¼é…ç½® JSON")

    args = parser.parse_args()

    print("ğŸ¯ VRçœ¼åŠ¨æ•°æ®è‡ªåŠ¨åˆ†æå·¥å…·")
    print("=" * 50)

    if not os.path.exists(args.input):
        print(f"[ERROR] è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input}")
        return

    video_files = get_video_files(args.input)
    if not video_files:
        print(f"[WARN] {args.input} ä¸­æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
        return

    print(f"ğŸ“ æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")

    analyzer = GazeAnalyzer()
    analyzer.black_threshold = args.black_threshold
    analyzer.detection_radius = args.radius
    if args.model:
        analyzer.load_model(args.model)

    print("\nè§†é¢‘æ–‡ä»¶åˆ—è¡¨:")
    for i, video_file in enumerate(video_files, 1):
        rel_path = os.path.relpath(video_file, args.input)
        print(f"{i:2d}. {rel_path}")

    try:
        choice = input(f"\nè¯·é€‰æ‹©è¦åˆ†æçš„è§†é¢‘ (1-{len(video_files)}, æˆ–è¾“å…¥ 'all' åˆ†æå…¨éƒ¨): ").strip()

        if choice.lower() == "all":
            selected_files = video_files
        else:
            choice_num = int(choice)
            if 1 <= choice_num <= len(video_files):
                selected_files = [video_files[choice_num - 1]]
            else:
                print("[ERROR] æ— æ•ˆé€‰æ‹©")
                return

        for video_file in selected_files:
            print(f"\nğŸš€ å¼€å§‹åˆ†æ {os.path.basename(video_file)}")
            segments = analyzer.analyze_video(
                video_file,
                args.output,
                show_preview=not args.no_preview
            )

            if segments:
                print(f"[DONE] æ£€æµ‹åˆ° {len(segments)} ä¸ªç‰‡æ®µ")
            else:
                print("[WARN] æœªæ£€æµ‹åˆ°æœ‰æ•ˆç‰‡æ®µ")

    except KeyboardInterrupt:
        print("\n[INFO] ç”¨æˆ·ä¸­æ–­")
    except ValueError:
        print("[ERROR] è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
    except Exception as exc:
        print(f"[ERROR] åˆ†æå¤±è´¥: {exc}")


if __name__ == "__main__":
    main()
