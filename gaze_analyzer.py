#!/usr/bin/env python3
# gaze_analyzer.py - VRçœ¼åŠ¨æ•°æ®è‡ªåŠ¨åˆ†æå·¥å…·
import cv2
import numpy as np
import os
import pandas as pd
from collections import defaultdict
import argparse
import glob

class GazeAnalyzer:
    def __init__(self, debug_mode=False):
        # æ£€æµ‹å‚æ•°
        self.black_threshold = 30  # é»‘è‰²é˜ˆå€¼ï¼ˆ0-255ï¼‰
        self.detection_radius = 20  # è§†çº¿ç‚¹å‘¨å›´æ£€æµ‹åŠå¾„
        self.min_duration = 5  # æœ€å°æŒç»­å¸§æ•°ï¼ˆé¿å…å™ªå£°ï¼‰
        
        # æ˜¾ç¤ºå‚æ•°
        self.indicator_size = (100, 80)  # æŒ‡ç¤ºå™¨å¤§å°
        self.indicator_pos = (20, 20)   # æŒ‡ç¤ºå™¨ä½ç½®
        
        # è°ƒè¯•æ¨¡å¼
        self.debug_mode = debug_mode
        
        # çŠ¶æ€è¿½è¸ª
        self.current_state = None  # 'reality' or 'virtual'
        self.state_start_frame = 0
        self.segments = []  # å­˜å‚¨æ‰€æœ‰ç‰‡æ®µ
        
    def detect_gaze_circle(self, frame):
        """æ£€æµ‹ç°ç™½è‰²åœ†å½¢è§†çº¿ç‚¹ï¼ˆæ’é™¤æ–¹å‘ç›˜æŒ‰é’®ï¼‰"""
        # è½¬æ¢ä¸ºç°åº¦å›¾
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # ä½¿ç”¨HoughCirclesæ£€æµ‹åœ†å½¢
        circles = cv2.HoughCircles(
            gray,
            cv2.HOUGH_GRADIENT,
            dp=1,
            minDist=50,
            param1=50,
            param2=30,
            minRadius=5,
            maxRadius=25
        )
        
        if circles is not None:
            circles = np.round(circles[0, :]).astype("int")
            
            # æ‰¾åˆ°çœŸæ­£çš„è§†çº¿ç‚¹ï¼ˆç°ç™½è‰²åœ†ï¼Œä¸æ˜¯æ–¹å‘ç›˜æŒ‰é’®ï¼‰
            best_circle = None
            max_score = 0
            all_circles_info = []  # ç”¨äºè°ƒè¯•æ˜¾ç¤º
            
            for (x, y, r) in circles:
                if 0 <= x < frame.shape[1] and 0 <= y < frame.shape[0]:
                    score = self.evaluate_gaze_circle(gray, x, y, r)
                    all_circles_info.append((x, y, r, score))
                    
                    if score > max_score:
                        max_score = score
                        best_circle = (x, y, r)
            
            # è°ƒè¯•æ¨¡å¼ï¼šåœ¨åŸå›¾ä¸Šæ˜¾ç¤ºæ‰€æœ‰æ£€æµ‹åˆ°çš„åœ†å½¢å’Œè¯„åˆ†
            if self.debug_mode and hasattr(self, '_debug_frame'):
                for (x, y, r, score) in all_circles_info:
                    color = (0, 255, 0) if score > 0.5 else (0, 0, 255)  # ç»¿è‰²=å¥½ï¼Œçº¢è‰²=å·®
                    cv2.circle(self._debug_frame, (x, y), r, color, 2)
                    cv2.putText(self._debug_frame, f"{score:.2f}", (x-20, y-r-10), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            
            # åªæœ‰å½“åˆ†æ•°è¶³å¤Ÿé«˜æ—¶æ‰è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„è§†çº¿ç‚¹
            gaze_threshold = getattr(self, 'gaze_threshold', 0.5)
            if max_score > gaze_threshold:
                return best_circle
        
        return None
    
    def evaluate_gaze_circle(self, gray, x, y, r):
        """è¯„ä¼°åœ†å½¢æ˜¯å¦ä¸ºè§†çº¿ç‚¹ï¼ˆè€Œä¸æ˜¯æ–¹å‘ç›˜æŒ‰é’®ï¼‰"""
        h, w = gray.shape
        
        # ç¡®ä¿æ£€æµ‹åŒºåŸŸåœ¨å›¾åƒèŒƒå›´å†…
        x1, y1 = max(0, x-r), max(0, y-r)
        x2, y2 = min(w, x+r), min(h, y+r)
        
        if x2 <= x1 or y2 <= y1:
            return 0
        
        # æå–åœ†å½¢åŒºåŸŸ
        roi = gray[y1:y2, x1:x2]
        
        # åˆ›å»ºåœ†å½¢æ©ç 
        mask = np.zeros(roi.shape, dtype=np.uint8)
        center_x, center_y = x - x1, y - y1
        cv2.circle(mask, (center_x, center_y), r, 255, -1)
        
        # åªåˆ†æåœ†å½¢åŒºåŸŸå†…çš„åƒç´ 
        circle_pixels = roi[mask > 0]
        
        if len(circle_pixels) == 0:
            return 0
        
        # è®¡ç®—æ•´ä½“äº®åº¦
        overall_brightness = np.mean(circle_pixels)
        
        # è®¡ç®—åœ†å¿ƒåŒºåŸŸçš„äº®åº¦ï¼ˆåŠå¾„çš„1/3ï¼‰
        inner_radius = max(1, r // 3)
        inner_mask = np.zeros(roi.shape, dtype=np.uint8)
        cv2.circle(inner_mask, (center_x, center_y), inner_radius, 255, -1)
        inner_pixels = roi[inner_mask > 0]
        
        if len(inner_pixels) == 0:
            return 0
        
        center_brightness = np.mean(inner_pixels)
        
        # è®¡ç®—åœ†ç¯åŒºåŸŸçš„äº®åº¦ï¼ˆå¤–ç¯ï¼‰
        outer_mask = mask.copy()
        cv2.circle(outer_mask, (center_x, center_y), inner_radius, 0, -1)
        ring_pixels = roi[outer_mask > 0]
        
        if len(ring_pixels) == 0:
            return 0
        
        ring_brightness = np.mean(ring_pixels)
        
        # è§†çº¿ç‚¹ç‰¹å¾ï¼š
        # 1. æ•´ä½“äº®åº¦è¾ƒé«˜ï¼ˆç°ç™½è‰²ï¼‰
        # 2. åœ†å¿ƒäº®åº¦ä¸åœ†ç¯äº®åº¦ç›¸è¿‘ï¼ˆå‡åŒ€çš„ç°ç™½è‰²ï¼‰
        # 3. ä¸æ˜¯é»‘å¿ƒç™½ç¯çš„ç»“æ„
        
        # æ–¹å‘ç›˜æŒ‰é’®ç‰¹å¾ï¼š
        # 1. åœ†å¿ƒå¾ˆæš—ï¼ˆé»‘è‰²ï¼‰
        # 2. åœ†ç¯è¾ƒäº®ï¼ˆç°è‰²ï¼‰
        # 3. ä¸­å¿ƒä¸å¤–ç¯äº®åº¦å·®å¼‚å¾ˆå¤§
        
        brightness_diff = abs(center_brightness - ring_brightness)
        brightness_ratio = center_brightness / (ring_brightness + 1)  # é¿å…é™¤é›¶
        
        score = 0
        
        # æ•´ä½“äº®åº¦è¯„åˆ†ï¼ˆè¶Šäº®è¶Šå¥½ï¼Œä½†ä¸èƒ½å¤ªäº®ï¼‰
        if 80 < overall_brightness < 200:
            score += 0.3
        elif overall_brightness >= 200:
            score += 0.1  # å¤ªäº®å¯èƒ½æ˜¯å™ªå£°
        
        # å‡åŒ€æ€§è¯„åˆ†ï¼ˆä¸­å¿ƒå’Œå¤–ç¯äº®åº¦åº”è¯¥ç›¸è¿‘ï¼‰
        if brightness_diff < 30:  # äº®åº¦å·®å¼‚å°
            score += 0.4
        
        # æ’é™¤é»‘å¿ƒç»“æ„ï¼ˆæ–¹å‘ç›˜æŒ‰é’®ï¼‰
        if center_brightness < 50:  # åœ†å¿ƒå¤ªæš—ï¼Œå¯èƒ½æ˜¯æ–¹å‘ç›˜æŒ‰é’®
            score -= 0.5
        
        # äº®åº¦æ¯”ä¾‹è¯„åˆ†ï¼ˆè§†çº¿ç‚¹çš„ä¸­å¿ƒä¸åº”è¯¥æ¯”å¤–ç¯æš—å¤ªå¤šï¼‰
        if brightness_ratio > 0.7:  # ä¸­å¿ƒäº®åº¦è‡³å°‘æ˜¯å¤–ç¯çš„70%
            score += 0.3
        
        return max(0, score)
    
    def analyze_gaze_region(self, frame, gaze_x, gaze_y):
        """åˆ†æè§†çº¿ç‚¹å‘¨å›´åŒºåŸŸåˆ¤æ–­æ˜¯ç°å®è¿˜æ˜¯è™šæ‹Ÿ"""
        h, w = frame.shape[:2]
        
        # ç¡®ä¿æ£€æµ‹åŒºåŸŸåœ¨å›¾åƒèŒƒå›´å†…
        x1 = max(0, gaze_x - self.detection_radius)
        y1 = max(0, gaze_y - self.detection_radius)
        x2 = min(w, gaze_x + self.detection_radius)
        y2 = min(h, gaze_y + self.detection_radius)
        
        # æå–æ£€æµ‹åŒºåŸŸ
        roi = frame[y1:y2, x1:x2]
        
        if roi.size == 0:
            return 'unknown'
        
        # è½¬æ¢ä¸ºç°åº¦å¹¶è®¡ç®—å¹³å‡äº®åº¦
        gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        avg_brightness = np.mean(gray_roi)
        
        # è®¡ç®—é»‘è‰²åƒç´ æ¯”ä¾‹
        black_pixels = np.sum(gray_roi < self.black_threshold)
        total_pixels = gray_roi.size
        black_ratio = black_pixels / total_pixels
        
        # åˆ¤æ–­é€»è¾‘ï¼šå¦‚æœé»‘è‰²åƒç´ æ¯”ä¾‹è¶…è¿‡50%æˆ–å¹³å‡äº®åº¦å¾ˆä½ï¼Œè®¤ä¸ºæ˜¯ç°å®ä¸–ç•Œ
        if black_ratio > 0.5 or avg_brightness < self.black_threshold:
            return 'reality'
        else:
            return 'virtual'
    
    def draw_indicator(self, frame, state):
        """åœ¨å·¦ä¸Šè§’ç»˜åˆ¶çŠ¶æ€æŒ‡ç¤ºå™¨"""
        x, y = self.indicator_pos
        w, h = self.indicator_size
        
        # é€‰æ‹©é¢œè‰²
        if state == 'reality':
            color = (0, 255, 0)  # ç»¿è‰²
            text = 'REALITY'
        elif state == 'virtual':
            color = (0, 0, 255)  # çº¢è‰²
            text = 'VIRTUAL'
        else:
            color = (128, 128, 128)  # ç°è‰²
            text = 'UNKNOWN'
        
        # ç»˜åˆ¶çŸ©å½¢æŒ‡ç¤ºå™¨
        cv2.rectangle(frame, (x, y), (x + w, y + h), color, -1)
        
        # æ·»åŠ æ–‡å­—
        font = cv2.FONT_HERSHEY_SIMPLEX
        text_size = cv2.getTextSize(text, font, 0.6, 2)[0]
        text_x = x + (w - text_size[0]) // 2
        text_y = y + (h + text_size[1]) // 2
        cv2.putText(frame, text, (text_x, text_y), font, 0.6, (255, 255, 255), 2)
    
    def update_state(self, new_state, frame_num, fps):
        """æ›´æ–°çŠ¶æ€å¹¶è®°å½•ç‰‡æ®µ"""
        if new_state != self.current_state:
            # çŠ¶æ€æ”¹å˜ï¼Œè®°å½•ä¸Šä¸€ä¸ªç‰‡æ®µ
            if self.current_state is not None and frame_num - self.state_start_frame >= self.min_duration:
                duration_frames = frame_num - self.state_start_frame
                duration_seconds = duration_frames / fps
                
                self.segments.append({
                    'state': self.current_state,
                    'start_frame': self.state_start_frame,
                    'end_frame': frame_num - 1,
                    'duration_frames': duration_frames,
                    'duration_seconds': duration_seconds,
                    'start_time': self.state_start_frame / fps,
                    'end_time': (frame_num - 1) / fps
                })
            
            # å¼€å§‹æ–°çŠ¶æ€
            self.current_state = new_state
            self.state_start_frame = frame_num
    
    def finalize_segments(self, total_frames, fps):
        """å®Œæˆæœ€åä¸€ä¸ªç‰‡æ®µçš„è®°å½•"""
        if self.current_state is not None and total_frames - self.state_start_frame >= self.min_duration:
            duration_frames = total_frames - self.state_start_frame
            duration_seconds = duration_frames / fps
            
            self.segments.append({
                'state': self.current_state,
                'start_frame': self.state_start_frame,
                'end_frame': total_frames - 1,
                'duration_frames': duration_frames,
                'duration_seconds': duration_seconds,
                'start_time': self.state_start_frame / fps,
                'end_time': (total_frames - 1) / fps
            })
    
    def analyze_video(self, video_path, output_dir=None, show_preview=True):
        """åˆ†æè§†é¢‘æ–‡ä»¶"""
        print(f"ğŸ¬ å¼€å§‹åˆ†æè§†é¢‘: {os.path.basename(video_path)}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            return None
        
        # è·å–è§†é¢‘ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps:.2f}fps, {total_frames}å¸§")
        
        # é‡ç½®çŠ¶æ€
        self.segments = []
        self.current_state = None
        
        frame_num = 0
        
        # å¦‚æœéœ€è¦ä¿å­˜å¤„ç†åçš„è§†é¢‘
        output_video = None
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            output_video_path = os.path.join(output_dir, f"{os.path.splitext(os.path.basename(video_path))[0]}_analyzed.mp4")
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # è®¾ç½®è°ƒè¯•å¸§å¼•ç”¨
                if self.debug_mode:
                    self._debug_frame = frame
                
                # æ£€æµ‹è§†çº¿ç‚¹
                gaze_circle = self.detect_gaze_circle(frame)
                
                current_state = 'unknown'
                if gaze_circle:
                    gaze_x, gaze_y, radius = gaze_circle
                    
                    # åˆ†æè§†çº¿åŒºåŸŸ
                    current_state = self.analyze_gaze_region(frame, gaze_x, gaze_y)
                    
                    # åœ¨è§†çº¿ç‚¹ç»˜åˆ¶åœ†åœˆï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    cv2.circle(frame, (gaze_x, gaze_y), radius, (255, 255, 0), 2)
                    cv2.circle(frame, (gaze_x, gaze_y), self.detection_radius, (0, 255, 255), 1)
                    
                    # æ˜¾ç¤ºæ£€æµ‹ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
                    cv2.putText(frame, f"Gaze Point", (gaze_x + radius + 5, gaze_y - 10), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
                
                # æ›´æ–°çŠ¶æ€
                self.update_state(current_state, frame_num, fps)
                
                # ç»˜åˆ¶çŠ¶æ€æŒ‡ç¤ºå™¨
                self.draw_indicator(frame, current_state)
                
                # æ˜¾ç¤ºè¿›åº¦
                if frame_num % 100 == 0:
                    progress = (frame_num / total_frames) * 100
                    print(f"â³ å¤„ç†è¿›åº¦: {progress:.1f}% ({frame_num}/{total_frames})")
                
                # ä¿å­˜å¤„ç†åçš„å¸§
                if output_video:
                    output_video.write(frame)
                
                # å®æ—¶é¢„è§ˆ
                if show_preview:
                    # ç¼©æ”¾æ˜¾ç¤ºï¼ˆå¦‚æœè§†é¢‘å¤ªå¤§ï¼‰
                    display_frame = frame
                    if width > 1280:
                        scale = 1280 / width
                        new_width = int(width * scale)
                        new_height = int(height * scale)
                        display_frame = cv2.resize(frame, (new_width, new_height))
                    
                    cv2.imshow('Gaze Analysis', display_frame)
                    
                    # æŒ‰ESCé€€å‡ºé¢„è§ˆ
                    if cv2.waitKey(1) & 0xFF == 27:
                        print("â¹ï¸  ç”¨æˆ·ä¸­æ–­é¢„è§ˆ")
                        break
                
                frame_num += 1
            
        finally:
            cap.release()
            if output_video:
                output_video.release()
            if show_preview:
                cv2.destroyAllWindows()
        
        # å®Œæˆæœ€åä¸€ä¸ªç‰‡æ®µ
        self.finalize_segments(frame_num, fps)
        
        print(f"âœ… åˆ†æå®Œæˆ! å…±å¤„ç† {frame_num} å¸§")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_report(video_path, output_dir)
        
        return self.segments
    
    def generate_report(self, video_path, output_dir):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not self.segments:
            print("âš ï¸  æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆç‰‡æ®µ")
            return
        
        # ç»Ÿè®¡æ•°æ®
        reality_segments = [s for s in self.segments if s['state'] == 'reality']
        virtual_segments = [s for s in self.segments if s['state'] == 'virtual']
        
        reality_duration = sum(s['duration_seconds'] for s in reality_segments)
        virtual_duration = sum(s['duration_seconds'] for s in virtual_segments)
        total_duration = reality_duration + virtual_duration
        
        print(f"\nğŸ“Š åˆ†ææŠ¥å‘Š:")
        print(f"=" * 50)
        print(f"ç°å®ä¸–ç•Œç‰‡æ®µ: {len(reality_segments)} ä¸ª, æ€»æ—¶é•¿: {reality_duration:.2f}ç§’")
        print(f"è™šæ‹Ÿä¸–ç•Œç‰‡æ®µ: {len(virtual_segments)} ä¸ª, æ€»æ—¶é•¿: {virtual_duration:.2f}ç§’")
        print(f"ç°å®ä¸–ç•Œå æ¯”: {(reality_duration/total_duration*100):.1f}%")
        print(f"è™šæ‹Ÿä¸–ç•Œå æ¯”: {(virtual_duration/total_duration*100):.1f}%")
        
        # ä¿å­˜è¯¦ç»†æ•°æ®
        if output_dir:
            # åˆ›å»ºDataFrame
            df_data = []
            for i, segment in enumerate(self.segments, 1):
                df_data.append({
                    'åºå·': i,
                    'çŠ¶æ€': 'ç°å®ä¸–ç•Œ' if segment['state'] == 'reality' else 'è™šæ‹Ÿä¸–ç•Œ',
                    'å¼€å§‹å¸§': segment['start_frame'],
                    'ç»“æŸå¸§': segment['end_frame'],
                    'æŒç»­å¸§æ•°': segment['duration_frames'],
                    'å¼€å§‹æ—¶é—´(ç§’)': round(segment['start_time'], 2),
                    'ç»“æŸæ—¶é—´(ç§’)': round(segment['end_time'], 2),
                    'æŒç»­æ—¶é—´(ç§’)': round(segment['duration_seconds'], 2)
                })
            
            df = pd.DataFrame(df_data)
            
            # ä¿å­˜CSVæ–‡ä»¶
            base_name = os.path.splitext(os.path.basename(video_path))[0]
            csv_path = os.path.join(output_dir, f"{base_name}_analysis.csv")
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            
            print(f"ğŸ“„ è¯¦ç»†æ•°æ®å·²ä¿å­˜: {csv_path}")

def get_video_files(directory):
    """è·å–ç›®å½•ä¸‹çš„è§†é¢‘æ–‡ä»¶"""
    video_extensions = ['*.mp4', '*.avi', '*.mov', '*.mkv', '*.flv']
    video_files = []
    
    for ext in video_extensions:
        video_files.extend(glob.glob(os.path.join(directory, '**', ext), recursive=True))
    
    return sorted(video_files)

def main():
    parser = argparse.ArgumentParser(description="VRçœ¼åŠ¨æ•°æ®è‡ªåŠ¨åˆ†æå·¥å…·")
    parser.add_argument("--input", "-i", default="çœ¼åŠ¨æ•°æ®", help="è¾“å…¥ç›®å½•ï¼ˆé»˜è®¤ï¼šçœ¼åŠ¨æ•°æ®ï¼‰")
    parser.add_argument("--output", "-o", default="analysis_results", help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šanalysis_resultsï¼‰")
    parser.add_argument("--no-preview", action="store_true", help="ä¸æ˜¾ç¤ºå®æ—¶é¢„è§ˆ")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆæ˜¾ç¤ºæ‰€æœ‰æ£€æµ‹åˆ°çš„åœ†å½¢å’Œè¯„åˆ†ï¼‰")
    parser.add_argument("--black-threshold", type=int, default=30, help="é»‘è‰²æ£€æµ‹é˜ˆå€¼ï¼ˆé»˜è®¤ï¼š30ï¼‰")
    parser.add_argument("--radius", type=int, default=20, help="æ£€æµ‹åŠå¾„ï¼ˆé»˜è®¤ï¼š20ï¼‰")
    parser.add_argument("--gaze-threshold", type=float, default=0.5, help="è§†çº¿ç‚¹è¯„åˆ†é˜ˆå€¼ï¼ˆé»˜è®¤ï¼š0.5ï¼‰")
    
    args = parser.parse_args()
    
    print("ğŸ¯ VRçœ¼åŠ¨æ•°æ®è‡ªåŠ¨åˆ†æå·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input}")
        return
    
    # è·å–è§†é¢‘æ–‡ä»¶
    video_files = get_video_files(args.input)
    
    if not video_files:
        print(f"âŒ åœ¨ {args.input} ä¸­æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = GazeAnalyzer(debug_mode=args.debug)
    analyzer.black_threshold = args.black_threshold
    analyzer.detection_radius = args.radius
    
    # æ›´æ–°è§†çº¿ç‚¹æ£€æµ‹é˜ˆå€¼
    analyzer.gaze_threshold = args.gaze_threshold
    
    if args.debug:
        print("ğŸ› è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
        print(f"   é»‘è‰²é˜ˆå€¼: {args.black_threshold}")
        print(f"   æ£€æµ‹åŠå¾„: {args.radius}")
        print(f"   è§†çº¿ç‚¹é˜ˆå€¼: {args.gaze_threshold}")
    
    # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨å¹¶è®©ç”¨æˆ·é€‰æ‹©
    print("\nè§†é¢‘æ–‡ä»¶åˆ—è¡¨:")
    for i, video_file in enumerate(video_files, 1):
        rel_path = os.path.relpath(video_file, args.input)
        print(f"{i:2d}. {rel_path}")
    
    try:
        choice = input(f"\nè¯·é€‰æ‹©è¦åˆ†æçš„è§†é¢‘ (1-{len(video_files)}, æˆ– 'all' åˆ†ææ‰€æœ‰): ").strip()
        
        if choice.lower() == 'all':
            selected_files = video_files
        else:
            choice_num = int(choice)
            if 1 <= choice_num <= len(video_files):
                selected_files = [video_files[choice_num - 1]]
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©")
                return
        
        # åˆ†æé€‰å®šçš„è§†é¢‘
        for video_file in selected_files:
            print(f"\nğŸš€ å¼€å§‹åˆ†æ: {os.path.basename(video_file)}")
            
            segments = analyzer.analyze_video(
                video_file, 
                args.output, 
                show_preview=not args.no_preview
            )
            
            if segments:
                print(f"âœ… åˆ†æå®Œæˆï¼Œå…±æ£€æµ‹åˆ° {len(segments)} ä¸ªç‰‡æ®µ")
            else:
                print("âŒ åˆ†æå¤±è´¥")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­")
    except ValueError:
        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    main()
