#!/usr/bin/env python3
"""
VRçœ¼åŠ¨æ•°æ®è‡ªåŠ¨åˆ†æå·¥å…· / VR Gaze Data Auto Analysis Tool

åŠŸèƒ½è¯´æ˜ï¼š
- è‡ªåŠ¨æ£€æµ‹è§†é¢‘ä¸­çš„ç™½è‰²åœ†å½¢è§†çº¿ç‚¹
- åˆ†æè§†çº¿ç‚¹å‘¨å›´åŒºåŸŸåˆ¤æ–­ç°å®ä¸–ç•Œvsè™šæ‹Ÿä¸–ç•Œ
- ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Šå’Œæ—¶é—´æ®µç»Ÿè®¡
- æ”¯æŒå®æ—¶é¢„è§ˆå’Œæ‰¹é‡å¤„ç†

ä½œè€…ï¼šWeihao
ç‰ˆæœ¬ï¼š1.0
æ–‡ä»¶åï¼šgaze_analyzer.py
"""
import cv2
import numpy as np
import os
import pandas as pd
from collections import defaultdict
import argparse
import glob

class GazeAnalyzer:
    def __init__(self):
        # æ£€æµ‹å‚æ•°
        self.black_threshold = 30  # é»‘è‰²é˜ˆå€¼ï¼ˆ0-255ï¼‰
        self.detection_radius = 20  # è§†çº¿ç‚¹å‘¨å›´æ£€æµ‹åŠå¾„
        self.min_duration = 5  # æœ€å°æŒç»­å¸§æ•°ï¼ˆé¿å…å™ªå£°ï¼‰
        
        # æ˜¾ç¤ºå‚æ•°
        self.indicator_size = (100, 80)  # æŒ‡ç¤ºå™¨å¤§å°
        self.indicator_pos = (20, 20)   # æŒ‡ç¤ºå™¨ä½ç½®
        
        # çŠ¶æ€è¿½è¸ª
        self.current_state = None  # 'reality' or 'virtual'
        self.state_start_frame = 0
        self.segments = []  # å­˜å‚¨æ‰€æœ‰ç‰‡æ®µ
        
    def detect_gaze_circle(self, frame):
        """æ£€æµ‹ç™½è‰²åœ†å½¢è§†çº¿ç‚¹"""
        # è½¬æ¢ä¸ºç°åº¦å›¾
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # è®¡ç®—æ’é™¤åŒºåŸŸè¾¹ç•Œ - æ’é™¤é¡¶éƒ¨5%å’Œå·¦å³ä¸¤ä¾§10%
        h, w = gray.shape
        top_exclude = int(h * 0.05)      # é¡¶éƒ¨5%
        left_exclude = int(w * 0.23)     # å·¦ä¾§10%
        right_exclude = w - int(w * 0.23) # å³ä¾§10%
        
        # å…ˆå°è¯•æ ‡å‡†å‚æ•°æ£€æµ‹
        circles = cv2.HoughCircles(
            gray,
            cv2.HOUGH_GRADIENT,
            dp=1,
            minDist=30,  # å‡å°æœ€å°è·ç¦»é¿å…æ–¹å‘ç›˜åœ†åœˆå¹²æ‰°
            param1=60,   # æé«˜è¾¹ç¼˜æ£€æµ‹é˜ˆå€¼
            param2=35,   # æé«˜ç´¯åŠ å™¨é˜ˆå€¼å‡å°‘è¯¯è¯†åˆ«
            minRadius=3, # å‡å°50%: 5->3
            maxRadius=12 # å‡å°50%: 25->12
        )
        
        # å¦‚æœæ ‡å‡†å‚æ•°æ²¡æ£€æµ‹åˆ°ï¼Œä¸”å›¾åƒè¾ƒæš—ï¼ˆå¯èƒ½æ˜¯é»‘è‰²èƒŒæ™¯ï¼‰ï¼Œä½¿ç”¨æ›´æ•æ„Ÿçš„å‚æ•°
        if circles is None:
            avg_brightness = np.mean(gray)
            if avg_brightness < 80:  # åˆ¤æ–­ä¸ºæš—èƒŒæ™¯
                circles = cv2.HoughCircles(
                    gray,
                    cv2.HOUGH_GRADIENT,
                    dp=1,
                    minDist=25,  # è¿›ä¸€æ­¥å‡å°è·ç¦»
                    param1=40,   # é™ä½è¾¹ç¼˜æ£€æµ‹é˜ˆå€¼ï¼Œå¢åŠ æ•æ„Ÿåº¦
                    param2=20,   # å¤§å¹…é™ä½ç´¯åŠ å™¨é˜ˆå€¼
                    minRadius=3,
                    maxRadius=12
                )
        
        if circles is not None:
            circles = np.round(circles[0, :]).astype("int")
            
            # æ‰¾åˆ°æœ€äº®çš„åœ†ï¼ˆå¯èƒ½æ˜¯è§†çº¿ç‚¹ï¼‰
            best_circle = None
            max_brightness = 0
            
            for (x, y, r) in circles:
                # æ£€æŸ¥æ˜¯å¦åœ¨æœ‰æ•ˆæ£€æµ‹åŒºåŸŸå†…ï¼ˆæ’é™¤é¡¶éƒ¨5%å’Œå·¦å³ä¸¤ä¾§10%ï¼‰
                if (left_exclude <= x <= right_exclude and 
                    y >= top_exclude and 
                    0 <= x < frame.shape[1] and 0 <= y < frame.shape[0]):
                    # æ£€æŸ¥åœ†å¿ƒå‘¨å›´çš„äº®åº¦ - å¢å¼ºé»‘è‰²åŒºåŸŸè¯†åˆ«èƒ½åŠ›
                    roi = gray[max(0, y-r):min(gray.shape[0], y+r),
                              max(0, x-r):min(gray.shape[1], x+r)]
                    if roi.size > 0:
                        brightness = np.mean(roi)
                        # å¢å¼ºå¯¹æ¯”åº¦æ£€æµ‹ï¼Œä¼˜å…ˆé€‰æ‹©ä¸å‘¨å›´å¯¹æ¯”åº¦é«˜çš„åœ†
                        surrounding_roi = gray[max(0, y-r*2):min(gray.shape[0], y+r*2),
                                             max(0, x-r*2):min(gray.shape[1], x+r*2)]
                        if surrounding_roi.size > 0:
                            contrast = brightness - np.mean(surrounding_roi)
                            
                            # é’ˆå¯¹é»‘è‰²èƒŒæ™¯ä¼˜åŒ–è¯„åˆ†ç­–ç•¥
                            avg_brightness = np.mean(gray)
                            if avg_brightness < 80:  # é»‘è‰²èƒŒæ™¯ä¸‹
                                # é»‘èƒŒæ™¯ä¸‹æ›´é‡è§†å¯¹æ¯”åº¦ï¼Œé™ä½äº®åº¦è¦æ±‚
                                score = brightness * 0.4 + contrast * 0.6
                                # é¢å¤–åŠ åˆ†ï¼šå¦‚æœæ˜¯çœŸæ­£çš„ç™½ç‚¹ï¼ˆäº®åº¦>150ä¸”å¯¹æ¯”åº¦>50ï¼‰
                                if brightness > 150 and contrast > 50:
                                    score += 50
                            else:  # æ­£å¸¸èƒŒæ™¯ä¸‹
                                score = brightness * 0.7 + contrast * 0.3
                                
                            if score > max_brightness:
                                max_brightness = score
                                best_circle = (x, y, r)
            
            return best_circle
        
        return None
    
    def analyze_gaze_region(self, frame, gaze_x, gaze_y):
        """åˆ†æè§†çº¿ç‚¹å‘¨å›´åŒºåŸŸåˆ¤æ–­æ˜¯ç°å®è¿˜æ˜¯è™šæ‹Ÿ"""
        h, w = frame.shape[:2]
        
        # ç¡®ä¿æ£€æµ‹åŒºåŸŸåœ¨å›¾åƒèŒƒå›´å†…
        x1 = max(0, gaze_x - self.detection_radius)
        y1 = max(0, gaze_y - self.detection_radius)
        x2 = min(w, gaze_x + self.detection_radius)
        y2 = min(h, gaze_y + self.detection_radius)
        
        # æå–æ£€æµ‹åŒºåŸŸ
        roi = frame[y1:y2, x1:x2]
        
        if roi.size == 0:
            return 'unknown'
        
        # è½¬æ¢ä¸ºç°åº¦å¹¶è®¡ç®—å¹³å‡äº®åº¦
        gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        avg_brightness = np.mean(gray_roi)
        
        # è®¡ç®—é»‘è‰²åƒç´ æ¯”ä¾‹
        black_pixels = np.sum(gray_roi < self.black_threshold)
        total_pixels = gray_roi.size
        black_ratio = black_pixels / total_pixels
        
        # åˆ¤æ–­é€»è¾‘ï¼šå¦‚æœé»‘è‰²åƒç´ æ¯”ä¾‹è¶…è¿‡50%æˆ–å¹³å‡äº®åº¦å¾ˆä½ï¼Œè®¤ä¸ºæ˜¯ç°å®ä¸–ç•Œ
        if black_ratio > 0.5 or avg_brightness < self.black_threshold:
            return 'reality'
        else:
            return 'virtual'
    
    def draw_indicator(self, frame, state):
        """åœ¨å·¦ä¸Šè§’ç»˜åˆ¶çŠ¶æ€æŒ‡ç¤ºå™¨"""
        x, y = self.indicator_pos
        w, h = self.indicator_size
        
        # é€‰æ‹©é¢œè‰²
        if state == 'reality':
            color = (0, 255, 0)  # ç»¿è‰²
            text = 'REALITY'
        elif state == 'virtual':
            color = (0, 0, 255)  # çº¢è‰²
            text = 'VIRTUAL'
        else:
            color = (128, 128, 128)  # ç°è‰²
            text = 'UNKNOWN'
        
        # ç»˜åˆ¶çŸ©å½¢æŒ‡ç¤ºå™¨
        cv2.rectangle(frame, (x, y), (x + w, y + h), color, -1)
        
        # æ·»åŠ æ–‡å­—
        font = cv2.FONT_HERSHEY_SIMPLEX
        text_size = cv2.getTextSize(text, font, 0.6, 2)[0]
        text_x = x + (w - text_size[0]) // 2
        text_y = y + (h + text_size[1]) // 2
        cv2.putText(frame, text, (text_x, text_y), font, 0.6, (255, 255, 255), 2)
    
    def update_state(self, new_state, frame_num, fps):
        """æ›´æ–°çŠ¶æ€å¹¶è®°å½•ç‰‡æ®µ"""
        if new_state != self.current_state:
            # çŠ¶æ€æ”¹å˜ï¼Œè®°å½•ä¸Šä¸€ä¸ªç‰‡æ®µ
            if self.current_state is not None and frame_num - self.state_start_frame >= self.min_duration:
                duration_frames = frame_num - self.state_start_frame
                duration_seconds = duration_frames / fps
                
                self.segments.append({
                    'state': self.current_state,
                    'start_frame': self.state_start_frame,
                    'end_frame': frame_num - 1,
                    'duration_frames': duration_frames,
                    'duration_seconds': duration_seconds,
                    'start_time': self.state_start_frame / fps,
                    'end_time': (frame_num - 1) / fps
                })
            
            # å¼€å§‹æ–°çŠ¶æ€
            self.current_state = new_state
            self.state_start_frame = frame_num
    
    def finalize_segments(self, total_frames, fps):
        """å®Œæˆæœ€åä¸€ä¸ªç‰‡æ®µçš„è®°å½•"""
        if self.current_state is not None and total_frames - self.state_start_frame >= self.min_duration:
            duration_frames = total_frames - self.state_start_frame
            duration_seconds = duration_frames / fps
            
            self.segments.append({
                'state': self.current_state,
                'start_frame': self.state_start_frame,
                'end_frame': total_frames - 1,
                'duration_frames': duration_frames,
                'duration_seconds': duration_seconds,
                'start_time': self.state_start_frame / fps,
                'end_time': (total_frames - 1) / fps
            })
    
    def analyze_video(self, video_path, output_dir=None, show_preview=True):
        """åˆ†æè§†é¢‘æ–‡ä»¶"""
        print(f"ğŸ¬ å¼€å§‹åˆ†æè§†é¢‘: {os.path.basename(video_path)}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            return None
        
        # è·å–è§†é¢‘ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps:.2f}fps, {total_frames}å¸§")
        
        # é‡ç½®çŠ¶æ€
        self.segments = []
        self.current_state = None
        
        frame_num = 0
        
        # å¦‚æœéœ€è¦ä¿å­˜å¤„ç†åçš„è§†é¢‘
        output_video = None
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            output_video_path = os.path.join(output_dir, f"{os.path.splitext(os.path.basename(video_path))[0]}_analyzed.mp4")
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # æ£€æµ‹è§†çº¿ç‚¹
                gaze_circle = self.detect_gaze_circle(frame)
                
                current_state = 'unknown'
                if gaze_circle:
                    gaze_x, gaze_y, radius = gaze_circle
                    
                    # åˆ†æè§†çº¿åŒºåŸŸ
                    current_state = self.analyze_gaze_region(frame, gaze_x, gaze_y)
                    
                    # åœ¨è§†çº¿ç‚¹ç»˜åˆ¶åœ†åœˆï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    cv2.circle(frame, (gaze_x, gaze_y), radius, (255, 255, 0), 2)
                    cv2.circle(frame, (gaze_x, gaze_y), self.detection_radius, (0, 255, 255), 1)
                
                # æ›´æ–°çŠ¶æ€
                self.update_state(current_state, frame_num, fps)
                
                # ç»˜åˆ¶çŠ¶æ€æŒ‡ç¤ºå™¨
                self.draw_indicator(frame, current_state)
                
                # æ˜¾ç¤ºè¿›åº¦
                if frame_num % 100 == 0:
                    progress = (frame_num / total_frames) * 100
                    print(f"â³ å¤„ç†è¿›åº¦: {progress:.1f}% ({frame_num}/{total_frames})")
                
                # ä¿å­˜å¤„ç†åçš„å¸§
                if output_video:
                    output_video.write(frame)
                
                # å®æ—¶é¢„è§ˆ
                if show_preview:
                    # ç¼©æ”¾æ˜¾ç¤ºï¼ˆå¦‚æœè§†é¢‘å¤ªå¤§ï¼‰
                    display_frame = frame
                    if width > 1280:
                        scale = 1280 / width
                        new_width = int(width * scale)
                        new_height = int(height * scale)
                        display_frame = cv2.resize(frame, (new_width, new_height))
                    
                    cv2.imshow('Gaze Analysis', display_frame)
                    
                    # æŒ‰ESCé€€å‡ºé¢„è§ˆ
                    if cv2.waitKey(1) & 0xFF == 27:
                        print("â¹ï¸  ç”¨æˆ·ä¸­æ–­é¢„è§ˆ")
                        break
                
                frame_num += 1
            
        finally:
            cap.release()
            if output_video:
                output_video.release()
            if show_preview:
                cv2.destroyAllWindows()
        
        # å®Œæˆæœ€åä¸€ä¸ªç‰‡æ®µ
        self.finalize_segments(frame_num, fps)
        
        print(f"âœ… åˆ†æå®Œæˆ! å…±å¤„ç† {frame_num} å¸§")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_report(video_path, output_dir)
        
        return self.segments
    
    def generate_report(self, video_path, output_dir):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not self.segments:
            print("âš ï¸  æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆç‰‡æ®µ")
            return
        
        # ç»Ÿè®¡æ•°æ®
        reality_segments = [s for s in self.segments if s['state'] == 'reality']
        virtual_segments = [s for s in self.segments if s['state'] == 'virtual']
        
        reality_duration = sum(s['duration_seconds'] for s in reality_segments)
        virtual_duration = sum(s['duration_seconds'] for s in virtual_segments)
        total_duration = reality_duration + virtual_duration
        
        print(f"\nğŸ“Š åˆ†ææŠ¥å‘Š:")
        print(f"=" * 50)
        print(f"ç°å®ä¸–ç•Œç‰‡æ®µ: {len(reality_segments)} ä¸ª, æ€»æ—¶é•¿: {reality_duration:.2f}ç§’")
        print(f"è™šæ‹Ÿä¸–ç•Œç‰‡æ®µ: {len(virtual_segments)} ä¸ª, æ€»æ—¶é•¿: {virtual_duration:.2f}ç§’")
        
        if total_duration > 0:
            print(f"ç°å®ä¸–ç•Œå æ¯”: {(reality_duration/total_duration*100):.1f}%")
            print(f"è™šæ‹Ÿä¸–ç•Œå æ¯”: {(virtual_duration/total_duration*100):.1f}%")
        
        # ä¿å­˜è¯¦ç»†æ•°æ®
        if output_dir:
            # åˆ›å»ºDataFrame
            df_data = []
            for i, segment in enumerate(self.segments, 1):
                df_data.append({
                    'åºå·': i,
                    'çŠ¶æ€': 'ç°å®ä¸–ç•Œ' if segment['state'] == 'reality' else 'è™šæ‹Ÿä¸–ç•Œ',
                    'å¼€å§‹å¸§': segment['start_frame'],
                    'ç»“æŸå¸§': segment['end_frame'],
                    'æŒç»­å¸§æ•°': segment['duration_frames'],
                    'å¼€å§‹æ—¶é—´(ç§’)': round(segment['start_time'], 2),
                    'ç»“æŸæ—¶é—´(ç§’)': round(segment['end_time'], 2),
                    'æŒç»­æ—¶é—´(ç§’)': round(segment['duration_seconds'], 2)
                })
            
            df = pd.DataFrame(df_data)
            
            # ä¿å­˜CSVæ–‡ä»¶
            base_name = os.path.splitext(os.path.basename(video_path))[0]
            csv_path = os.path.join(output_dir, f"{base_name}_analysis.csv")
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            
            print(f"ğŸ“„ è¯¦ç»†æ•°æ®å·²ä¿å­˜: {csv_path}")

def get_video_files(directory):
    """è·å–ç›®å½•ä¸‹çš„è§†é¢‘æ–‡ä»¶"""
    video_extensions = ['*.mp4', '*.avi', '*.mov', '*.mkv', '*.flv']
    video_files = []
    
    for ext in video_extensions:
        video_files.extend(glob.glob(os.path.join(directory, '**', ext), recursive=True))
    
    return sorted(video_files)

def main():
    parser = argparse.ArgumentParser(description="VRçœ¼åŠ¨æ•°æ®è‡ªåŠ¨åˆ†æå·¥å…·")
    parser.add_argument("--input", "-i", default="çœ¼åŠ¨æ•°æ®", help="è¾“å…¥ç›®å½•ï¼ˆé»˜è®¤ï¼šçœ¼åŠ¨æ•°æ®ï¼‰")
    parser.add_argument("--output", "-o", default="analysis_results", help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šanalysis_resultsï¼‰")
    parser.add_argument("--no-preview", action="store_true", help="ä¸æ˜¾ç¤ºå®æ—¶é¢„è§ˆ")
    parser.add_argument("--black-threshold", type=int, default=30, help="é»‘è‰²æ£€æµ‹é˜ˆå€¼ï¼ˆé»˜è®¤ï¼š30ï¼‰")
    parser.add_argument("--radius", type=int, default=20, help="æ£€æµ‹åŠå¾„ï¼ˆé»˜è®¤ï¼š20ï¼‰")
    
    args = parser.parse_args()
    
    print("ğŸ¯ VRçœ¼åŠ¨æ•°æ®è‡ªåŠ¨åˆ†æå·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input}")
        return
    
    # è·å–è§†é¢‘æ–‡ä»¶
    video_files = get_video_files(args.input)
    
    if not video_files:
        print(f"âŒ åœ¨ {args.input} ä¸­æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = GazeAnalyzer()
    analyzer.black_threshold = args.black_threshold
    analyzer.detection_radius = args.radius
    
    # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨å¹¶è®©ç”¨æˆ·é€‰æ‹©
    print("\nè§†é¢‘æ–‡ä»¶åˆ—è¡¨:")
    for i, video_file in enumerate(video_files, 1):
        rel_path = os.path.relpath(video_file, args.input)
        print(f"{i:2d}. {rel_path}")
    
    try:
        choice = input(f"\nè¯·é€‰æ‹©è¦åˆ†æçš„è§†é¢‘ (1-{len(video_files)}, æˆ– 'all' åˆ†ææ‰€æœ‰): ").strip()
        
        if choice.lower() == 'all':
            selected_files = video_files
        else:
            choice_num = int(choice)
            if 1 <= choice_num <= len(video_files):
                selected_files = [video_files[choice_num - 1]]
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©")
                return
        
        # åˆ†æé€‰å®šçš„è§†é¢‘
        for video_file in selected_files:
            print(f"\nğŸš€ å¼€å§‹åˆ†æ: {os.path.basename(video_file)}")
            
            segments = analyzer.analyze_video(
                video_file, 
                args.output, 
                show_preview=not args.no_preview
            )
            
            if segments:
                print(f"âœ… åˆ†æå®Œæˆï¼Œå…±æ£€æµ‹åˆ° {len(segments)} ä¸ªç‰‡æ®µ")
            else:
                print("âŒ åˆ†æå¤±è´¥")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­")
    except ValueError:
        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    main()
