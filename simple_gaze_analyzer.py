#!/usr/bin/env python3
# simple_gaze_analyzer.py - ç®€åŒ–ç‰ˆVRçœ¼åŠ¨åˆ†æå·¥å…·ï¼ˆåªæ£€æµ‹é»‘è‰²åŒºåŸŸï¼‰
import cv2
import numpy as np
import os
import pandas as pd
from collections import defaultdict
import argparse
import glob

class SimpleGazeAnalyzer:
    def __init__(self, debug_mode=False):
        # æ£€æµ‹å‚æ•°
        self.black_threshold = 50  # é»‘è‰²é˜ˆå€¼ï¼ˆè°ƒé«˜ä¸€äº›ï¼Œæ›´å®½å®¹ï¼‰
        self.detection_radius = 30  # æ£€æµ‹åŠå¾„ï¼ˆåŠ å¤§èŒƒå›´ï¼‰
        self.min_duration = 10  # æœ€å°æŒç»­å¸§æ•°
        
        # æ˜¾ç¤ºå‚æ•°
        self.indicator_size = (120, 90)
        self.indicator_pos = (20, 20)
        
        # è°ƒè¯•æ¨¡å¼
        self.debug_mode = debug_mode
        
        # çŠ¶æ€è¿½è¸ª
        self.current_state = None
        self.state_start_frame = 0
        self.segments = []
        
        # è§†çº¿ç‚¹ä½ç½®ï¼ˆå¦‚æœæ£€æµ‹ä¸åˆ°ï¼Œä½¿ç”¨å±å¹•ä¸­å¿ƒï¼‰
        self.fallback_gaze_pos = None
    
    def detect_gaze_point(self, frame):
        """åœ¨çº¯é»‘èƒŒæ™¯ä¸‹ä¾èµ–åœ†å½¢æ£€æµ‹è§†çº¿ç‚¹"""
        h, w = frame.shape[:2]
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # æ­¥éª¤1ï¼šå®šä¹‰çº¯é»‘è‰²åŒºåŸŸï¼ˆè°ƒæ•´é˜ˆå€¼ï¼Œå¯èƒ½ä¹‹å‰å¤ªä¸¥æ ¼ï¼‰
        pure_black_mask = gray <= 30  # æ”¾å®½é»‘è‰²é˜ˆå€¼
        
        if self.debug_mode:
            black_pixel_count = np.sum(pure_black_mask)
            total_pixels = h * w
            black_percentage = (black_pixel_count / total_pixels) * 100
            print(f"é»‘è‰²åŒºåŸŸå æ¯”: {black_percentage:.1f}%")
        
        # æ­¥éª¤2ï¼šåœ¨é»‘è‰²åŒºåŸŸä¸­æ£€æµ‹åŠé€æ˜åœ†å½¢
        if np.any(pure_black_mask):
            # åˆ›å»ºåªåŒ…å«é»‘è‰²åŒºåŸŸçš„å›¾åƒ
            black_region_gray = gray.copy()
            black_region_gray[~pure_black_mask] = 0
            
            # é’ˆå¯¹åŠé€æ˜åœ†åœˆçš„å›¾åƒé¢„å¤„ç†
            # 1. è½»å¾®çš„é«˜æ–¯æ¨¡ç³Šï¼Œå‡å°‘å™ªå£°
            blurred = cv2.GaussianBlur(black_region_gray, (3, 3), 0)
            
            # 2. å¯¹æ¯”åº¦å¢å¼ºï¼Œè®©åŠé€æ˜åœ†åœˆæ›´æ˜æ˜¾
            enhanced = cv2.convertScaleAbs(blurred, alpha=1.5, beta=10)
            
            # 3. ä½¿ç”¨å¢å¼ºåçš„å›¾åƒè¿›è¡Œåœ†å½¢æ£€æµ‹
            circles = cv2.HoughCircles(
                enhanced,
                cv2.HOUGH_GRADIENT,
                dp=1,
                minDist=15,   # è¿›ä¸€æ­¥å‡å°æœ€å°è·ç¦»
                param1=8,     # éå¸¸ä½çš„è¾¹ç¼˜æ£€æµ‹é˜ˆå€¼
                param2=6,     # éå¸¸ä½çš„åœ†å¿ƒæ£€æµ‹é˜ˆå€¼
                minRadius=2,  # æœ€å°åŠå¾„
                maxRadius=40  # æœ€å¤§åŠå¾„
            )
            
            if self.debug_mode:
                circle_count = len(circles[0]) if circles is not None else 0
                print(f"æ£€æµ‹åˆ° {circle_count} ä¸ªåœ†å½¢")
            
            if circles is not None:
                circles = np.round(circles[0, :]).astype("int")
                
                best_circle = None
                max_score = 0
                
                for i, (x, y, r) in enumerate(circles):
                    if 0 <= x < w and 0 <= y < h:
                        # ç¡®ä¿åœ†å¿ƒåœ¨é»‘è‰²åŒºåŸŸå†…
                        if pure_black_mask[y, x]:
                            # è®¡ç®—åœ†çš„äº®åº¦
                            roi = gray[max(0, y-r):min(h, y+r), max(0, x-r):min(w, x+r)]
                            brightness = np.mean(roi) if roi.size > 0 else 0
                            
                            # æ£€æŸ¥å‘¨å›´é»‘è‰²æ¯”ä¾‹
                            check_radius = r * 2
                            roi_mask = pure_black_mask[max(0, y-check_radius):min(h, y+check_radius), 
                                                      max(0, x-check_radius):min(w, x+check_radius)]
                            black_ratio = np.sum(roi_mask) / roi_mask.size if roi_mask.size > 0 else 0
                            
                            # åŠé€æ˜åœ†åœˆçš„è¯„åˆ†æ ‡å‡†
                            # 1. ç›¸å¯¹äº®åº¦ï¼ˆåœ¨é»‘è‰²èƒŒæ™¯ä¸­ç›¸å¯¹è¾ƒäº®å³å¯ï¼‰
                            relative_brightness = brightness / 255
                            
                            # 2. æ£€æŸ¥æ˜¯å¦æ˜¯åœ†å½¢è¾¹ç¼˜æ¸…æ™°ï¼ˆåŠé€æ˜åœ†åœˆè¾¹ç¼˜å¯èƒ½æ¨¡ç³Šï¼‰
                            # è®¡ç®—åœ†å½¢åŒºåŸŸçš„æ ‡å‡†å·®ï¼ˆåŠé€æ˜åœ†åœˆæ ‡å‡†å·®è¾ƒå°ï¼‰
                            roi = gray[max(0, y-r):min(h, y+r), max(0, x-r):min(w, x+r)]
                            brightness_std = np.std(roi) if roi.size > 0 else 0
                            
                            # 3. æ£€æŸ¥åœ†å¿ƒæ˜¯å¦æ¯”å‘¨å›´äº®
                            center_brightness = gray[y, x] if 0 <= x < w and 0 <= y < h else 0
                            
                            # è¯„åˆ†ï¼šé™ä½äº®åº¦è¦æ±‚ï¼Œé‡è§†ç›¸å¯¹äº®åº¦å’Œä½ç½®
                            score = 0
                            
                            # åŸºç¡€äº®åº¦è¯„åˆ†ï¼ˆåŠé€æ˜åœ†åœˆäº®åº¦å¯èƒ½åªæœ‰30-100ï¼‰
                            if brightness > 25:  # å¤§å¹…é™ä½äº®åº¦è¦æ±‚
                                score += min(relative_brightness * 2, 0.4)  # æœ€å¤š0.4åˆ†
                            
                            # é»‘è‰²ç¯å¢ƒè¯„åˆ†
                            if black_ratio > 0.5:
                                score += 0.3
                            
                            # åœ†å¿ƒäº®åº¦è¯„åˆ†ï¼ˆåœ†å¿ƒåº”è¯¥ç›¸å¯¹è¾ƒäº®ï¼‰
                            if center_brightness > brightness * 0.8:
                                score += 0.2
                            
                            # ä½ç½®åˆç†æ€§ï¼ˆä¸è¦å¤ªé è¾¹ï¼‰
                            margin = min(w, h) * 0.1
                            if margin < x < w-margin and margin < y < h-margin:
                                score += 0.1
                            
                            if self.debug_mode:
                                print(f"åœ†å½¢{i+1}: ä½ç½®({x},{y}), åŠå¾„{r}, äº®åº¦{brightness:.1f}, ä¸­å¿ƒäº®åº¦{center_brightness:.1f}, é»‘è‰²æ¯”ä¾‹{black_ratio:.2f}, è¯„åˆ†{score:.2f}")
                            
                            if score > max_score:
                                max_score = score
                                best_circle = (x, y, r)
                
                if best_circle and max_score > 0.3:  # é™ä½è¯„åˆ†é˜ˆå€¼
                    if self.debug_mode:
                        print(f"âœ… é€‰æ‹©è§†çº¿ç‚¹: {best_circle[:2]}, è¯„åˆ†: {max_score:.2f}")
                    return best_circle[:2]
            
            # å¤‡ç”¨æ–¹æ³•ï¼šå¦‚æœåœ†å½¢æ£€æµ‹å¤±è´¥ï¼Œå¯»æ‰¾é»‘è‰²åŒºåŸŸä¸­çš„æœ€äº®ç‚¹
            if self.debug_mode:
                print("âš ï¸  åœ†å½¢æ£€æµ‹å¤±è´¥ï¼Œå°è¯•å¯»æ‰¾æœ€äº®ç‚¹...")
            
            # åœ¨é»‘è‰²åŒºåŸŸä¸­æ‰¾æœ€äº®çš„åƒç´ 
            max_brightness = np.max(black_region_gray)
            if max_brightness > 20:  # è¿›ä¸€æ­¥é™ä½äº®åº¦è¦æ±‚
                bright_locations = np.where(black_region_gray == max_brightness)
                if len(bright_locations[0]) > 0:
                    # é€‰æ‹©æœ€æ¥è¿‘ä¸­å¿ƒçš„äº®ç‚¹
                    center_x, center_y = w//2, h//2
                    min_distance = float('inf')
                    best_point = None
                    
                    for i in range(len(bright_locations[0])):
                        y_pos = bright_locations[0][i]
                        x_pos = bright_locations[1][i]
                        distance = np.sqrt((x_pos - center_x)**2 + (y_pos - center_y)**2)
                        
                        if distance < min_distance:
                            min_distance = distance
                            best_point = (x_pos, y_pos)
                    
                    if best_point and self.debug_mode:
                        print(f"âœ… æ‰¾åˆ°æœ€äº®ç‚¹: {best_point}, äº®åº¦: {max_brightness}")
                    
                    return best_point
            
            if self.debug_mode:
                print("âŒ é»‘è‰²åŒºåŸŸä¸­æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿäº®çš„ç‚¹")
        
        else:
            if self.debug_mode:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°é»‘è‰²åŒºåŸŸ")
        
        return None
    
    def is_in_black_region(self, gaze_pos):
        """ç®€åŒ–åˆ¤æ–­ï¼šå¦‚æœåœ¨é»‘è‰²åŒºåŸŸæ‰¾åˆ°äº†è§†çº¿ç‚¹ï¼Œå°±æ˜¯ç°å®ä¸–ç•Œ"""
        return gaze_pos is not None
    
    def draw_indicator(self, frame, state, gaze_pos=None):
        """ç»˜åˆ¶çŠ¶æ€æŒ‡ç¤ºå™¨å’Œæ£€æµ‹ä¿¡æ¯"""
        x, y = self.indicator_pos
        w, h = self.indicator_size
        
        # çŠ¶æ€æŒ‡ç¤ºï¼š
        # ğŸŸ¢ ç»¿è‰²çŸ©å½¢ = ç°å®ä¸–ç•Œï¼ˆé»‘è‰²åŒºåŸŸï¼‰
        # ğŸ”´ çº¢è‰²çŸ©å½¢ = è™šæ‹Ÿä¸–ç•Œï¼ˆæ¸¸æˆåŒºåŸŸï¼‰
        if state == 'reality':
            color = (0, 255, 0)  # ç»¿è‰² - ç°å®ä¸–ç•Œï¼ˆé»‘è‰²åŒºåŸŸï¼‰
            text = 'REALITY'
            text_color = (0, 0, 0)  # é»‘è‰²æ–‡å­—
        elif state == 'virtual':
            color = (0, 0, 255)  # çº¢è‰² - è™šæ‹Ÿä¸–ç•Œï¼ˆæ¸¸æˆåŒºåŸŸï¼‰
            text = 'VIRTUAL'
            text_color = (255, 255, 255)  # ç™½è‰²æ–‡å­—
        else:
            color = (128, 128, 128)  # ç°è‰² - æœªçŸ¥çŠ¶æ€
            text = 'UNKNOWN'
            text_color = (255, 255, 255)
        
        # ç»˜åˆ¶çŠ¶æ€æŒ‡ç¤ºå™¨çŸ©å½¢
        cv2.rectangle(frame, (x, y), (x + w, y + h), color, -1)
        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 255, 255), 3)
        
        # æ·»åŠ çŠ¶æ€æ–‡å­—
        font = cv2.FONT_HERSHEY_SIMPLEX
        text_size = cv2.getTextSize(text, font, 0.8, 2)[0]
        text_x = x + (w - text_size[0]) // 2
        text_y = y + (h + text_size[1]) // 2
        cv2.putText(frame, text, (text_x, text_y), font, 0.8, text_color, 2)
        
        # ç»˜åˆ¶è§†çº¿ç‚¹ï¼ˆå¦‚æœæ£€æµ‹åˆ°ï¼‰
        if gaze_pos:
            gaze_x, gaze_y = gaze_pos
            
            # è§†çº¿ç‚¹åœ†å½¢æ ‡è®°ï¼ˆé»„è‰²åœ†åœˆï¼‰
            cv2.circle(frame, (gaze_x, gaze_y), 12, (0, 255, 255), 3)  # é»„è‰²å¤–åœˆ
            cv2.circle(frame, (gaze_x, gaze_y), 4, (0, 255, 255), -1)  # é»„è‰²å®å¿ƒä¸­å¿ƒ
            
            # æ˜¾ç¤ºåæ ‡ä¿¡æ¯
            info_text = f"Gaze: ({gaze_x},{gaze_y})"
            cv2.putText(frame, info_text, (gaze_x + 20, gaze_y - 20), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        else:
            # æ²¡æœ‰æ£€æµ‹åˆ°è§†çº¿ç‚¹æ—¶çš„æç¤º
            no_gaze_text = "No gaze in black region"
            cv2.putText(frame, no_gaze_text, (x, y + h + 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    def update_state(self, new_state, frame_num, fps):
        """æ›´æ–°çŠ¶æ€å¹¶è®°å½•ç‰‡æ®µ"""
        if new_state != self.current_state:
            # è®°å½•ä¸Šä¸€ä¸ªç‰‡æ®µ
            if self.current_state is not None and frame_num - self.state_start_frame >= self.min_duration:
                duration_frames = frame_num - self.state_start_frame
                duration_seconds = duration_frames / fps
                
                self.segments.append({
                    'state': self.current_state,
                    'start_frame': self.state_start_frame,
                    'end_frame': frame_num - 1,
                    'duration_frames': duration_frames,
                    'duration_seconds': duration_seconds,
                    'start_time': self.state_start_frame / fps,
                    'end_time': (frame_num - 1) / fps
                })
            
            # å¼€å§‹æ–°çŠ¶æ€
            self.current_state = new_state
            self.state_start_frame = frame_num
    
    def finalize_segments(self, total_frames, fps):
        """å®Œæˆæœ€åä¸€ä¸ªç‰‡æ®µ"""
        if self.current_state is not None and total_frames - self.state_start_frame >= self.min_duration:
            duration_frames = total_frames - self.state_start_frame
            duration_seconds = duration_frames / fps
            
            self.segments.append({
                'state': self.current_state,
                'start_frame': self.state_start_frame,
                'end_frame': total_frames - 1,
                'duration_frames': duration_frames,
                'duration_seconds': duration_seconds,
                'start_time': self.state_start_frame / fps,
                'end_time': (total_frames - 1) / fps
            })
    
    def analyze_video(self, video_path, output_dir=None, show_preview=True):
        """åˆ†æè§†é¢‘æ–‡ä»¶"""
        print(f"ğŸ¬ å¼€å§‹åˆ†æè§†é¢‘: {os.path.basename(video_path)}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            return None
        
        # è·å–è§†é¢‘ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps:.2f}fps, {total_frames}å¸§")
        
        # é‡ç½®çŠ¶æ€
        self.segments = []
        self.current_state = None
        
        frame_num = 0
        
        # è¾“å‡ºè§†é¢‘è®¾ç½®
        output_video = None
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            output_video_path = os.path.join(output_dir, f"{os.path.splitext(os.path.basename(video_path))[0]}_analyzed.mp4")
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # æ£€æµ‹è§†çº¿ç‚¹ä½ç½®ï¼ˆåªåœ¨çº¯é»‘è‰²åŒºåŸŸä¸­å¯»æ‰¾ï¼‰
                gaze_pos = self.detect_gaze_point(frame)
                
                # åˆ¤æ–­çŠ¶æ€ï¼šèƒ½åœ¨é»‘è‰²åŒºåŸŸæ‰¾åˆ°è§†çº¿ç‚¹ = ç°å®ä¸–ç•Œ
                if gaze_pos:
                    current_state = 'reality'  # åœ¨é»‘è‰²åŒºåŸŸæ‰¾åˆ°äº†è§†çº¿ç‚¹
                else:
                    current_state = 'virtual'  # æ²¡æœ‰åœ¨é»‘è‰²åŒºåŸŸæ‰¾åˆ°è§†çº¿ç‚¹
                    gaze_pos = (width//2, height//2)  # æ˜¾ç¤ºç”¨çš„fallbackä½ç½®
                
                # æ›´æ–°çŠ¶æ€
                self.update_state(current_state, frame_num, fps)
                
                # ç»˜åˆ¶æŒ‡ç¤ºå™¨å’Œè°ƒè¯•ä¿¡æ¯
                self.draw_indicator(frame, current_state, gaze_pos)
                
                # è°ƒè¯•æ¨¡å¼ï¼šæ˜¾ç¤ºå¤„ç†è¿‡ç¨‹
                if self.debug_mode:
                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    pure_black_mask = gray <= 30
                    
                    # å³ä¸Šè§’æ˜¾ç¤ºé»‘è‰²æ©ç 
                    mask_display = pure_black_mask.astype(np.uint8) * 255
                    mask_small = cv2.resize(mask_display, (150, 100))
                    mask_colored = cv2.cvtColor(mask_small, cv2.COLOR_GRAY2BGR)
                    
                    # å³ä¸Šè§’ç¬¬ä¸€ä¸ªçª—å£ï¼šé»‘è‰²æ©ç 
                    start_y1, start_x1 = 20, width - 160
                    end_y1, end_x1 = start_y1 + 100, start_x1 + 150
                    
                    if end_x1 <= width and end_y1 <= height:
                        frame[start_y1:end_y1, start_x1:end_x1] = mask_colored
                        cv2.putText(frame, "Black Mask", (start_x1, start_y1 - 5), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
                    
                    # å³ä¸Šè§’ç¬¬äºŒä¸ªçª—å£ï¼šå¢å¼ºåçš„é»‘è‰²åŒºåŸŸ
                    black_region_gray = gray.copy()
                    black_region_gray[~pure_black_mask] = 0
                    blurred = cv2.GaussianBlur(black_region_gray, (3, 3), 0)
                    enhanced = cv2.convertScaleAbs(blurred, alpha=1.5, beta=10)
                    
                    enhanced_small = cv2.resize(enhanced, (150, 100))
                    enhanced_colored = cv2.cvtColor(enhanced_small, cv2.COLOR_GRAY2BGR)
                    
                    start_y2, start_x2 = 130, width - 160
                    end_y2, end_x2 = start_y2 + 100, start_x2 + 150
                    
                    if end_x2 <= width and end_y2 <= height:
                        frame[start_y2:end_y2, start_x2:end_x2] = enhanced_colored
                        cv2.putText(frame, "Enhanced", (start_x2, start_y2 - 5), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
                
                # æ˜¾ç¤ºè¿›åº¦
                if frame_num % 100 == 0:
                    progress = (frame_num / total_frames) * 100
                    print(f"â³ å¤„ç†è¿›åº¦: {progress:.1f}% ({frame_num}/{total_frames})")
                
                # ä¿å­˜å¤„ç†åçš„å¸§
                if output_video:
                    output_video.write(frame)
                
                # å®æ—¶é¢„è§ˆ
                if show_preview:
                    display_frame = frame
                    if width > 1280:
                        scale = 1280 / width
                        new_width = int(width * scale)
                        new_height = int(height * scale)
                        display_frame = cv2.resize(frame, (new_width, new_height))
                    
                    cv2.imshow('Simple Gaze Analysis', display_frame)
                    
                    key = cv2.waitKey(1) & 0xFF
                    if key == 27:  # ESCé€€å‡º
                        print("â¹ï¸  ç”¨æˆ·ä¸­æ–­é¢„è§ˆ")
                        break
                    elif key == ord(' '):  # ç©ºæ ¼æš‚åœ
                        cv2.waitKey(0)
                
                frame_num += 1
            
        finally:
            cap.release()
            if output_video:
                output_video.release()
            if show_preview:
                cv2.destroyAllWindows()
        
        # å®Œæˆåˆ†æ
        self.finalize_segments(frame_num, fps)
        print(f"âœ… åˆ†æå®Œæˆ! å…±å¤„ç† {frame_num} å¸§")
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(video_path, output_dir)
        return self.segments
    
    def generate_report(self, video_path, output_dir):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not self.segments:
            print("âš ï¸  æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆç‰‡æ®µ")
            return
        
        # ç»Ÿè®¡æ•°æ®
        reality_segments = [s for s in self.segments if s['state'] == 'reality']
        virtual_segments = [s for s in self.segments if s['state'] == 'virtual']
        
        reality_duration = sum(s['duration_seconds'] for s in reality_segments)
        virtual_duration = sum(s['duration_seconds'] for s in virtual_segments)
        total_duration = reality_duration + virtual_duration
        
        print(f"\nğŸ“Š åˆ†ææŠ¥å‘Š:")
        print(f"=" * 50)
        print(f"ç°å®ä¸–ç•Œç‰‡æ®µ: {len(reality_segments)} ä¸ª, æ€»æ—¶é•¿: {reality_duration:.2f}ç§’")
        print(f"è™šæ‹Ÿä¸–ç•Œç‰‡æ®µ: {len(virtual_segments)} ä¸ª, æ€»æ—¶é•¿: {virtual_duration:.2f}ç§’")
        if total_duration > 0:
            print(f"ç°å®ä¸–ç•Œå æ¯”: {(reality_duration/total_duration*100):.1f}%")
            print(f"è™šæ‹Ÿä¸–ç•Œå æ¯”: {(virtual_duration/total_duration*100):.1f}%")
        
        # ä¿å­˜è¯¦ç»†æ•°æ®
        if output_dir:
            df_data = []
            for i, segment in enumerate(self.segments, 1):
                df_data.append({
                    'åºå·': i,
                    'çŠ¶æ€': 'ç°å®ä¸–ç•Œ' if segment['state'] == 'reality' else 'è™šæ‹Ÿä¸–ç•Œ',
                    'å¼€å§‹å¸§': segment['start_frame'],
                    'ç»“æŸå¸§': segment['end_frame'],
                    'æŒç»­å¸§æ•°': segment['duration_frames'],
                    'å¼€å§‹æ—¶é—´(ç§’)': round(segment['start_time'], 2),
                    'ç»“æŸæ—¶é—´(ç§’)': round(segment['end_time'], 2),
                    'æŒç»­æ—¶é—´(ç§’)': round(segment['duration_seconds'], 2)
                })
            
            df = pd.DataFrame(df_data)
            base_name = os.path.splitext(os.path.basename(video_path))[0]
            csv_path = os.path.join(output_dir, f"{base_name}_simple_analysis.csv")
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"ğŸ“„ è¯¦ç»†æ•°æ®å·²ä¿å­˜: {csv_path}")

def main():
    parser = argparse.ArgumentParser(description="ç®€åŒ–ç‰ˆVRçœ¼åŠ¨æ•°æ®åˆ†æå·¥å…·")
    parser.add_argument("--input", "-i", default="çœ¼åŠ¨æ•°æ®", help="è¾“å…¥ç›®å½•")
    parser.add_argument("--output", "-o", default="simple_analysis_results", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--no-preview", action="store_true", help="ä¸æ˜¾ç¤ºé¢„è§ˆ")
    parser.add_argument("--debug", action="store_true", help="è°ƒè¯•æ¨¡å¼")
    parser.add_argument("--black-threshold", type=int, default=50, help="é»‘è‰²é˜ˆå€¼")
    parser.add_argument("--radius", type=int, default=30, help="æ£€æµ‹åŠå¾„")
    
    args = parser.parse_args()
    
    print("ğŸ¯ ç®€åŒ–ç‰ˆVRçœ¼åŠ¨æ•°æ®åˆ†æå·¥å…·")
    print("=" * 50)
    print("ä¸“æ³¨äºæ£€æµ‹è§†çº¿ç‚¹æ˜¯å¦åœ¨é»‘è‰²åŒºåŸŸ")
    
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input}")
        return
    
    # è·å–è§†é¢‘æ–‡ä»¶
    video_extensions = ['*.mp4', '*.avi', '*.mov', '*.mkv']
    video_files = []
    for ext in video_extensions:
        video_files.extend(glob.glob(os.path.join(args.input, '**', ext), recursive=True))
    
    if not video_files:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = SimpleGazeAnalyzer(debug_mode=args.debug)
    analyzer.black_threshold = args.black_threshold
    analyzer.detection_radius = args.radius
    
    # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
    print("\nè§†é¢‘æ–‡ä»¶åˆ—è¡¨:")
    for i, video_file in enumerate(video_files, 1):
        rel_path = os.path.relpath(video_file, args.input)
        print(f"{i:2d}. {rel_path}")
    
    try:
        choice = input(f"\nè¯·é€‰æ‹©è¦åˆ†æçš„è§†é¢‘ (1-{len(video_files)}, æˆ– 'all'): ").strip()
        
        if choice.lower() == 'all':
            selected_files = video_files
        else:
            choice_num = int(choice)
            if 1 <= choice_num <= len(video_files):
                selected_files = [video_files[choice_num - 1]]
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©")
                return
        
        # åˆ†æè§†é¢‘
        for video_file in selected_files:
            print(f"\nğŸš€ å¼€å§‹åˆ†æ: {os.path.basename(video_file)}")
            segments = analyzer.analyze_video(
                video_file, 
                args.output, 
                show_preview=not args.no_preview
            )
            
            if segments:
                print(f"âœ… åˆ†æå®Œæˆï¼Œå…±æ£€æµ‹åˆ° {len(segments)} ä¸ªç‰‡æ®µ")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­")
    except ValueError:
        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    main()
